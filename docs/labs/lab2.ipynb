{"cells":[{"cell_type":"markdown","metadata":{"id":"Oa09Had3Ldfq"},"source":["# Installing MASE (again)\n","\n","Run the block below to install MASE in the current Colab runtime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ClqAsevLdJx"},"outputs":[],"source":["git_token = \"\"\n","short_code = \"jaredjoss\"\n","\n","# Check the current python version (It should be using Python 3.10) and update pip to the latest version.\n","!python --version\n","!python -m pip install --user --upgrade pip\n","\n","# Clone MASE from your branch (the branch must already exist)\n","!git clone -b lab1_{short_code} https://{git_token}@github.com/DeepWok/mase.git\n","\n","# Install requirements\n","!python -m pip install -r ./mase/machop/requirements.txt\n","\n","# Change working directory to machop\n","%cd ./mase/machop/"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/Users/jared/Documents/Personal/ICL/courses/ADLS/mase/machop\n"]}],"source":["%cd ../../machop/"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Gg2xBbRmYZ1g"},"outputs":[{"name":"stdout","output_type":"stream","text":["usage: ch [--config PATH] [--task TASK] [--load PATH] [--load-type]\n","          [--batch-size NUM] [--debug] [--log-level]\n","          [--report-to {wandb,tensorboard}] [--seed NUM] [--quant-config TOML]\n","          [--training-optimizer TYPE] [--trainer-precision TYPE]\n","          [--learning-rate NUM] [--weight-decay NUM] [--max-epochs NUM]\n","          [--max-steps NUM] [--accumulate-grad-batches NUM]\n","          [--log-every-n-steps NUM] [--cpu NUM] [--gpu NUM] [--nodes NUM]\n","          [--accelerator TYPE] [--strategy TYPE] [--auto-requeue]\n","          [--github-ci] [--disable-dataset-cache] [--target STR]\n","          [--num-targets NUM] [--pretrained] [--max-token-len NUM]\n","          [--project-dir DIR] [--project NAME] [-h] [-V] [--info [TYPE]]\n","          action [model] [dataset]\n","\n","Chop is a simple utility, part of the MASE tookit, to train, test and\n","transform (i.e. prune or quantise) a supported model.\n","\n","main arguments:\n","  action                action to perform. One of\n","                        (train|test|transform|search)\n","  model                 name of a supported model. Required if configuration\n","                        NOT provided.\n","  dataset               name of a supported dataset. Required if configuration\n","                        NOT provided.\n","\n","general options:\n","  --config PATH         path to a configuration file in the TOML format.\n","                        Manual CLI overrides for arguments have a higher\n","                        precedence. Required if the action is transform.\n","                        (default: None)\n","  --task TASK           task to perform. One of (classification|cls|translatio\n","                        n|tran|language_modeling|lm) (default: classification)\n","  --load PATH           path to load the model from. (default: None)\n","  --load-type           the type of checkpoint to be loaded; it's disregarded\n","                        if --load is NOT specified. It is designed to and must\n","                        be used in tandem with --load. One of (pt|pl|mz|hf)\n","                        (default: mz)\n","  --batch-size NUM      batch size for training and evaluation. (default: 128)\n","  --debug               run the action in debug mode, which enables verbose\n","                        logging, custom exception hook that uses ipdb, and\n","                        sets the PL trainer to run in \"fast_dev_run\" mode.\n","                        (default: False)\n","  --log-level           verbosity level of the logger; it's only effective\n","                        when --debug flag is NOT passed in. One of\n","                        (debug|info|warning|error|critical) (default: info)\n","  --report-to {wandb,tensorboard}\n","                        reporting tool for logging metrics. One of\n","                        (wandb|tensorboard) (default: tensorboard)\n","  --seed NUM            seed for random number generators set via Pytorch\n","                        Lightning's seed_everything function. (default: 0)\n","  --quant-config TOML   path to a configuration file in the TOML format.\n","                        Manual CLI overrides for arguments have a higher\n","                        precedence. (default: None)\n","\n","trainer options:\n","  --training-optimizer TYPE\n","                        name of supported optimiser for training. One of\n","                        (adam|sgd|adamw) (default: adam)\n","  --trainer-precision TYPE\n","                        numeric precision for training. One of\n","                        (16-mixed|32|64|bf16) (default: 16-mixed)\n","  --learning-rate NUM   initial learning rate for training. (default: 1e-05)\n","  --weight-decay NUM    weight decay for training. (default: 0)\n","  --max-epochs NUM      maximum number of epochs for training. (default: 20)\n","  --max-steps NUM       maximum number of steps for training. A negative value\n","                        disables this option. (default: -1)\n","  --accumulate-grad-batches NUM\n","                        number of batches to accumulate gradients. (default:\n","                        1)\n","  --log-every-n-steps NUM\n","                        log every n steps. No logs if num_batches <\n","                        log_every_n_steps. (default: 50))\n","\n","runtime environment options:\n","  --cpu NUM, --num-workers NUM\n","                        number of CPU workers; the default varies across\n","                        systems and is set to os.cpu_count(). (default: 8)\n","  --gpu NUM, --num-devices NUM\n","                        number of GPU devices. (default: 1)\n","  --nodes NUM           number of nodes. (default: 1)\n","  --accelerator TYPE    type of accelerator for training. One of\n","                        (auto|cpu|gpu|mps) (default: auto)\n","  --strategy TYPE       type of strategy for training. One of\n","                        (auto|ddp|ddp_find_unused_parameters_true) (default:\n","                        auto)\n","  --auto-requeue        enable automatic job resubmission on SLURM managed\n","                        cluster. (default: False)\n","  --github-ci           set the execution environment to GitHub's CI pipeline;\n","                        it's used in the MASE verilog emitter transform pass\n","                        to skip simulations. (default: False)\n","  --disable-dataset-cache\n","                        disable caching of datasets. (default: False)\n","\n","hardware generation options:\n","  --target STR          target FPGA for hardware synthesis. (default:\n","                        xcu250-figd2104-2L-e)\n","  --num-targets NUM     number of FPGA devices. (default: 100)\n","\n","language model options:\n","  --pretrained          load pretrained checkpoint from\n","                        HuggingFace/Torchvision when initialising models.\n","                        (default: False)\n","  --max-token-len NUM   maximum number of tokens. A negative value will use\n","                        tokenizer.model_max_length. (default: 512)\n","\n","project options:\n","  --project-dir DIR     directory to save the project to. (default: /Users/jar\n","                        ed/Documents/Personal/ICL/courses/ADLS/mase/mase_outpu\n","                        t)\n","  --project NAME        name of the project. (default: {MODEL-NAME}_{TASK-\n","                        TYPE}_{DATASET-NAME}_{TIMESTAMP})\n","\n","information:\n","  -h, --help            show this help message and exit\n","  -V, --version         show version and exit\n","  --info [TYPE]         list information about supported models or/and\n","                        datasets and exit. One of (all|model|dataset)\n","                        (default: all)\n","\n","Maintained by the DeepWok Lab. Raise issues at\n","https://github.com/JianyiCheng/mase-tools/issues\n"]}],"source":["!./ch --help"]},{"cell_type":"markdown","metadata":{"id":"VF0krky2N4dr"},"source":["# General introduction\n","\n","In this lab, you will learn how to use the software stack of MASE. There are in total 7 tasks you would need to finish, and 1 optional task."]},{"cell_type":"markdown","metadata":{"id":"DIOZ48aaMqhV"},"source":["# Turning you network to a graph\n","\n","One specific feature of MASE is its capability to transform DL models to a computation graph using the [torch.fx](<https://pytorch.org/docs/stable/fx.html>) framework.\n"]},{"cell_type":"markdown","metadata":{"id":"cjBbOikbLJdN"},"source":["## Use the Transform functionality without CLI\n","\n","This tutorial describes how to use the MASE transform functionality for a pre-trained model."]},{"cell_type":"markdown","metadata":{"id":"j5sg0OpYLJdO"},"source":["## Import related packages and machop"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"4KO86-UYLJdP"},"outputs":[],"source":["import sys\n","import logging\n","import os\n","from pathlib import Path\n","from pprint import pprint as pp"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"l7BE0UToLJdQ"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[32mINFO    \u001b[0m \u001b[34mSet logging level to debug\u001b[0m\n"]}],"source":["from chop.dataset import MaseDataModule, get_dataset_info\n","from chop.tools.logger import set_logging_verbosity\n","\n","from chop.passes.graph import (\n","    save_node_meta_param_interface_pass,\n","    report_node_meta_param_analysis_pass,\n","    profile_statistics_analysis_pass,\n","    add_common_metadata_analysis_pass,\n","    init_metadata_analysis_pass,\n","    add_software_metadata_analysis_pass,\n",")\n","from chop.tools.get_input import InputGenerator\n","from chop.tools.checkpoint_load import load_model\n","from chop.ir import MaseGraph\n","\n","from chop.models import get_model_info, get_model\n","\n","set_logging_verbosity(\"debug\")"]},{"cell_type":"markdown","metadata":{"id":"q35_tV4QLJdQ"},"source":["## Set up the dataset\n","\n","Here we create a `MaseDataModule` using the `jsc` dataset from lab1. Note the `MaseDataModule` also requires the name of the model you plan to use data module with. In this case it is `jsc-tiny`."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"ztEzclWrLJdR"},"outputs":[],"source":["batch_size = 8\n","model_name = \"jsc-tiny\"\n","dataset_name = \"jsc\"\n","\n","\n","data_module = MaseDataModule(\n","    name=dataset_name,\n","    batch_size=batch_size,\n","    model_name=model_name,\n","    num_workers=0,\n",")\n","data_module.prepare_data()\n","data_module.setup()\n"]},{"cell_type":"markdown","metadata":{"id":"08-4XdNYLJdR"},"source":["## Set up the model \n","\n","Here we use the previously trained `jsc-tiny` model in lab 1 as an example."]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# # If you stored your model checkpoint on Google Drive, remember to mount the drive to the current runtime in order to access it\n","# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"gIU4s9CiLJdR"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /Users/jared/Documents/Personal/ICL/courses/ADLS/mase/mase_output/jsc-tiny_classification_jsc_2024-01-23/software/training_ckpts/best-v5.ckpt\u001b[0m\n"]}],"source":["# üìùÔ∏è change this CHECKPOINT_PATH to the one you trained in Lab1\n","CHECKPOINT_PATH = \"/Users/jared/Documents/Personal/ICL/courses/ADLS/mase/mase_output/jsc-tiny_classification_jsc_2024-01-23/software/training_ckpts/best-v5.ckpt\"\n","model_info = get_model_info(model_name)\n","model = get_model(\n","    model_name,\n","    task=\"cls\",\n","    dataset_info=data_module.dataset_info,\n","    pretrained=False)\n","\n","model = load_model(load_name=CHECKPOINT_PATH, load_type=\"pl\", model=model)"]},{"cell_type":"markdown","metadata":{"id":"lrIWs8XvLJdR"},"source":["# Get a dummy data in\n","With the dataset module and model information, we can grab an input generator."]},{"cell_type":"code","execution_count":27,"metadata":{"id":"ygyzBWJyLJdR"},"outputs":[],"source":["# get the input generator\n","input_generator = InputGenerator(\n","    data_module=data_module,\n","    model_info=model_info,\n","    task=\"cls\",\n","    which_dataloader=\"train\",\n",")\n","\n","# a demonstration of how to feed an input value to the model\n","dummy_in = next(iter(input_generator))\n","_ = model(**dummy_in)\n"]},{"cell_type":"markdown","metadata":{"id":"-sucUgAVLJdS"},"source":["## Generate a MaseGraph\n","We have two forms of passes: transform passes and analysis passes, both of them would require the model to be transferred into a MaseGraph to allow manipulation."]},{"cell_type":"code","execution_count":28,"metadata":{"id":"wSmp6oaGLJdS"},"outputs":[],"source":["# generate the mase graph and initialize node metadata\n","mg = MaseGraph(model=model)"]},{"cell_type":"markdown","metadata":{"id":"SiEehxhNLJdS"},"source":["## Running an Analysis pass\n","Analysis pass DOES NOT change the graph\n","\n","The following analysis passes are essential to prepare the graph for other passes"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"UuXo32cJLJdS"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[36mDEBUG   \u001b[0m \u001b[34mgraph():\n","    %x : [num_users=1] = placeholder[target=x]\n","    %seq_blocks_0 : [num_users=1] = call_module[target=seq_blocks.0](args = (%x,), kwargs = {})\n","    %seq_blocks_1 : [num_users=1] = call_module[target=seq_blocks.1](args = (%seq_blocks_0,), kwargs = {})\n","    %seq_blocks_2 : [num_users=1] = call_module[target=seq_blocks.2](args = (%seq_blocks_1,), kwargs = {})\n","    %seq_blocks_3 : [num_users=1] = call_module[target=seq_blocks.3](args = (%seq_blocks_2,), kwargs = {})\n","    return seq_blocks_3\u001b[0m\n"]}],"source":["mg, _ = init_metadata_analysis_pass(mg, None)\n","mg, _ = add_common_metadata_analysis_pass(mg, {\"dummy_in\": dummy_in})\n","mg, _ = add_software_metadata_analysis_pass(mg, None)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"text/plain":["<chop.ir.graph.mase_graph.MaseGraph at 0x2a70f48e0>"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["mg"]},{"cell_type":"markdown","metadata":{"id":"bAE-LFypLJdS"},"source":["We will first run a simple graph analysis to understand the structure of the model."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"V4c2sgqdLJdS"},"outputs":[{"name":"stdout","output_type":"stream","text":["graph():\n","    %x : [num_users=1] = placeholder[target=x]\n","    %seq_blocks_0 : [num_users=1] = call_module[target=seq_blocks.0](args = (%x,), kwargs = {})\n","    %seq_blocks_1 : [num_users=1] = call_module[target=seq_blocks.1](args = (%seq_blocks_0,), kwargs = {})\n","    %seq_blocks_2 : [num_users=1] = call_module[target=seq_blocks.2](args = (%seq_blocks_1,), kwargs = {})\n","    %seq_blocks_3 : [num_users=1] = call_module[target=seq_blocks.3](args = (%seq_blocks_2,), kwargs = {})\n","    return seq_blocks_3\n","Network overview:\n","{'placeholder': 1, 'get_attr': 0, 'call_function': 0, 'call_method': 0, 'call_module': 4, 'output': 1}\n","Layer types:\n","[BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Linear(in_features=16, out_features=5, bias=True), ReLU(inplace=True)]\n"]}],"source":["# report graph is an analysis pass that shows you the detailed information in the graph\n","from chop.passes.graph import report_graph_analysis_pass\n","_ = report_graph_analysis_pass(mg)"]},{"cell_type":"markdown","metadata":{"id":"GjIq-hnWLJdS"},"source":["## Running another Analysis pass: Profile statistics\n","\n","The pass `profile_statistics_analysis_pass` collects statistics of parameters and activations, and save them to node's metadata.\n","\n","Here is a list of all the supported statistics. Refer to the `__init__` of statistic classes in `chop.passes.analysis.statistical_profiler.stat` to check the args each stat class takes.\n","\n","This is a more complex analysis than the previous pass, and thus it would require you to pass in additional arguments for this pass.\n","\n","### Example: the range of weights & input activations of nodes\n","\n","Say we want to collect the tensor-wise min-max range of the 1st `torch.nn.Linear` nodes' weights & bias, and the channel-wise 97% quantile min-max of the 1st `torch.nn.Linear` nodes' input activations. We can do the following:"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"wTcTTDV8LJdS"},"outputs":[],"source":["pass_args = {\n","    \"by\": \"type\",                                                            # collect statistics by node name\n","    \"target_weight_nodes\": [\"linear\"],                                       # collect weight statistics for linear layers\n","    \"target_activation_nodes\": [\"relu\"],                                     # collect activation statistics for relu layers\n","    \"weight_statistics\": {\n","        \"variance_precise\": {\"device\": \"cpu\", \"dims\": \"all\"},                # collect precise variance of the weight\n","    },\n","    \"activation_statistics\": {\n","        \"range_quantile\": {\"device\": \"cpu\", \"dims\": \"all\", \"quantile\": 0.97} # collect 97% quantile of the activation range\n","    },\n","    \"input_generator\": input_generator,                                      # the input generator for feeding data to the model\n","    \"num_samples\": 32,                                                       # feed 32 samples to the model\n","}"]},{"cell_type":"markdown","metadata":{"id":"NhxtJNCVLJdT"},"source":["We can use the `report_node_meta_param_analysis_pass` to inspect the collected statistics."]},{"cell_type":"code","execution_count":33,"metadata":{"id":"OGAXEK29LJdT"},"outputs":[{"name":"stderr","output_type":"stream","text":["Profiling weight statistics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 12169.16it/s]\n","Profiling act statistics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 705.93it/s]\n"]}],"source":["mg, _ = profile_statistics_analysis_pass(mg, pass_args)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[32mINFO    \u001b[0m \u001b[34mInspecting graph [add_common_meta_param_analysis_pass]\u001b[0m\n","\u001b[32mINFO    \u001b[0m \u001b[34m\n","+--------------+--------------+---------------------+--------------+----------------------------------------------------------------------------------------+\n","| Node name    | Fx Node op   | Mase type           | Mase op      | Software Param                                                                         |\n","+==============+==============+=====================+==============+========================================================================================+\n","| x            | placeholder  | placeholder         | placeholder  | {'results': {'data_out_0': {'stat': {}}}}                                              |\n","+--------------+--------------+---------------------+--------------+----------------------------------------------------------------------------------------+\n","| seq_blocks_0 | call_module  | module              | batch_norm1d | {'args': {'bias': {'stat': {}},                                                        |\n","|              |              |                     |              |           'data_in_0': {'stat': {}},                                                   |\n","|              |              |                     |              |           'running_mean': {'stat': {}},                                                |\n","|              |              |                     |              |           'running_var': {'stat': {}},                                                 |\n","|              |              |                     |              |           'weight': {'stat': {}}},                                                     |\n","|              |              |                     |              |  'results': {'data_out_0': {'stat': {}}}}                                              |\n","+--------------+--------------+---------------------+--------------+----------------------------------------------------------------------------------------+\n","| seq_blocks_1 | call_module  | module_related_func | relu         | {'args': {'data_in_0': {'stat': {'range_quantile': {'count': 512,                      |\n","|              |              |                     |              |                                                     'max': 1.623390793800354,          |\n","|              |              |                     |              |                                                     'min': -0.9905546307563782,        |\n","|              |              |                     |              |                                                     'range': 2.613945484161377}}}},    |\n","|              |              |                     |              |  'results': {'data_out_0': {'stat': {}}}}                                              |\n","+--------------+--------------+---------------------+--------------+----------------------------------------------------------------------------------------+\n","| seq_blocks_2 | call_module  | module_related_func | linear       | {'args': {'bias': {'stat': {'variance_precise': {'count': 5,                           |\n","|              |              |                     |              |                                                  'mean': 0.16319331526756287,          |\n","|              |              |                     |              |                                                  'variance': 0.12900599837303162}}},   |\n","|              |              |                     |              |           'data_in_0': {'stat': {}},                                                   |\n","|              |              |                     |              |           'weight': {'stat': {'variance_precise': {'count': 80,                        |\n","|              |              |                     |              |                                                    'mean': 0.038789670914411545,       |\n","|              |              |                     |              |                                                    'variance': 0.0977274477481842}}}}, |\n","|              |              |                     |              |  'results': {'data_out_0': {'stat': {}}}}                                              |\n","+--------------+--------------+---------------------+--------------+----------------------------------------------------------------------------------------+\n","| seq_blocks_3 | call_module  | module_related_func | relu         | {'args': {'data_in_0': {'stat': {'range_quantile': {'count': 160,                      |\n","|              |              |                     |              |                                                     'max': 2.1779749393463135,         |\n","|              |              |                     |              |                                                     'min': -2.3127658367156982,        |\n","|              |              |                     |              |                                                     'range': 4.490740776062012}}}},    |\n","|              |              |                     |              |  'results': {'data_out_0': {'stat': {}}}}                                              |\n","+--------------+--------------+---------------------+--------------+----------------------------------------------------------------------------------------+\n","| output       | output       | output              | output       | {'args': {'data_in_0': {'stat': {}}}}                                                  |\n","+--------------+--------------+---------------------+--------------+----------------------------------------------------------------------------------------+\u001b[0m\n"]}],"source":["mg, _ = report_node_meta_param_analysis_pass(mg, {\"which\": (\"software\",)})"]},{"cell_type":"markdown","metadata":{"id":"6330lcJfLJdT"},"source":["## Running a Transform pass: Quantisation\n","\n","As its name suggests, the transform pass would modify the `MaseGraph`.\n","Similar to the previous analysis pass example, we would need to first declare the configuration for the pass."]},{"cell_type":"code","execution_count":35,"metadata":{"id":"t3MO3JYQLJdT"},"outputs":[],"source":["pass_args = {\n","    \"by\": \"type\",\n","    \"default\": {\"config\": {\"name\": None}},\n","    \"linear\": {\n","        \"config\": {\n","            \"name\": \"integer\",\n","            # data\n","            \"data_in_width\": 8,\n","            \"data_in_frac_width\": 4,\n","            # weight\n","            \"weight_width\": 8,\n","            \"weight_frac_width\": 4,\n","            # bias\n","            \"bias_width\": 8,\n","            \"bias_frac_width\": 4,\n","        }\n","    },\n","}"]},{"cell_type":"markdown","metadata":{"id":"OLuqdnuLLJdT"},"source":["We can then proceed to apply the transformation, in this case, we kept the original graph on purpose, so that we can print a `diff`."]},{"cell_type":"code","execution_count":36,"metadata":{"id":"zPiJQvs4LJdT"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[36mDEBUG   \u001b[0m \u001b[34mgraph():\n","    %x : [num_users=1] = placeholder[target=x]\n","    %seq_blocks_0 : [num_users=1] = call_module[target=seq_blocks.0](args = (%x,), kwargs = {})\n","    %seq_blocks_1 : [num_users=1] = call_module[target=seq_blocks.1](args = (%seq_blocks_0,), kwargs = {})\n","    %seq_blocks_2 : [num_users=1] = call_module[target=seq_blocks.2](args = (%seq_blocks_1,), kwargs = {})\n","    %seq_blocks_3 : [num_users=1] = call_module[target=seq_blocks.3](args = (%seq_blocks_2,), kwargs = {})\n","    return seq_blocks_3\u001b[0m\n","\u001b[36mDEBUG   \u001b[0m \u001b[34mCompare nodes:\u001b[0m\n","\u001b[36mDEBUG   \u001b[0m \u001b[34m\n","| Ori name     | New name     | MASE_TYPE           | Mase_OP      | Original type   | Quantized type   | Changed   |\n","|--------------+--------------+---------------------+--------------+-----------------+------------------+-----------|\n","| x            | x            | placeholder         | placeholder  | x               | x                | False     |\n","| seq_blocks_0 | seq_blocks_0 | module              | batch_norm1d | BatchNorm1d     | BatchNorm1d      | False     |\n","| seq_blocks_1 | seq_blocks_1 | module_related_func | relu         | ReLU            | ReLU             | False     |\n","| seq_blocks_2 | seq_blocks_2 | module_related_func | linear       | Linear          | LinearInteger    | True      |\n","| seq_blocks_3 | seq_blocks_3 | module_related_func | relu         | ReLU            | ReLU             | False     |\n","| output       | output       | output              | output       | output          | output           | False     |\u001b[0m\n","\u001b[32mINFO    \u001b[0m \u001b[34mQuantized graph histogram:\u001b[0m\n","\u001b[32mINFO    \u001b[0m \u001b[34m\n","| Original type   | OP           |   Total |   Changed |   Unchanged |\n","|-----------------+--------------+---------+-----------+-------------|\n","| BatchNorm1d     | batch_norm1d |       1 |         0 |           1 |\n","| Linear          | linear       |       1 |         1 |           0 |\n","| ReLU            | relu         |       2 |         0 |           2 |\n","| output          | output       |       1 |         0 |           1 |\n","| x               | placeholder  |       1 |         0 |           1 |\u001b[0m\n"]}],"source":["from chop.passes.graph.transforms import (\n","    quantize_transform_pass,\n","    summarize_quantization_analysis_pass,\n",")\n","from chop.ir.graph.mase_graph import MaseGraph\n","\n","\n","ori_mg = MaseGraph(model=model)\n","ori_mg, _ = init_metadata_analysis_pass(ori_mg, None)\n","ori_mg, _ = add_common_metadata_analysis_pass(ori_mg, {\"dummy_in\": dummy_in})\n","\n","mg, _ = quantize_transform_pass(mg, pass_args)\n","summarize_quantization_analysis_pass(ori_mg, mg, save_dir=\"quantize_summary\")"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[36mDEBUG   \u001b[0m \u001b[34mCompare nodes:\u001b[0m\n","\u001b[36mDEBUG   \u001b[0m \u001b[34m\n","| Ori name     | New name     | MASE_TYPE           | Mase_OP      | Original type   | Quantized type   | Changed   |\n","|--------------+--------------+---------------------+--------------+-----------------+------------------+-----------|\n","| x            | x            | placeholder         | placeholder  | x               | x                | False     |\n","| seq_blocks_0 | seq_blocks_0 | module              | batch_norm1d | BatchNorm1d     | BatchNorm1d      | False     |\n","| seq_blocks_1 | seq_blocks_1 | module_related_func | relu         | ReLU            | ReLU             | False     |\n","| seq_blocks_2 | seq_blocks_2 | module_related_func | linear       | Linear          | LinearInteger    | True      |\n","| seq_blocks_3 | seq_blocks_3 | module_related_func | relu         | ReLU            | ReLU             | False     |\n","| output       | output       | output              | output       | output          | output           | False     |\u001b[0m\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Ori name</th>\n","      <th>New name</th>\n","      <th>MASE_TYPE</th>\n","      <th>Mase_OP</th>\n","      <th>Original type</th>\n","      <th>Quantized type</th>\n","      <th>Changed</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>placeholder</td>\n","      <td>placeholder</td>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>seq_blocks_0</td>\n","      <td>seq_blocks_0</td>\n","      <td>module</td>\n","      <td>batch_norm1d</td>\n","      <td>BatchNorm1d</td>\n","      <td>BatchNorm1d</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>seq_blocks_1</td>\n","      <td>seq_blocks_1</td>\n","      <td>module_related_func</td>\n","      <td>relu</td>\n","      <td>ReLU</td>\n","      <td>ReLU</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>seq_blocks_2</td>\n","      <td>seq_blocks_2</td>\n","      <td>module_related_func</td>\n","      <td>linear</td>\n","      <td>Linear</td>\n","      <td>LinearInteger</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>seq_blocks_3</td>\n","      <td>seq_blocks_3</td>\n","      <td>module_related_func</td>\n","      <td>relu</td>\n","      <td>ReLU</td>\n","      <td>ReLU</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>output</td>\n","      <td>output</td>\n","      <td>output</td>\n","      <td>output</td>\n","      <td>output</td>\n","      <td>output</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Ori name      New name            MASE_TYPE       Mase_OP  \\\n","0             x             x          placeholder   placeholder   \n","1  seq_blocks_0  seq_blocks_0               module  batch_norm1d   \n","2  seq_blocks_1  seq_blocks_1  module_related_func          relu   \n","3  seq_blocks_2  seq_blocks_2  module_related_func        linear   \n","4  seq_blocks_3  seq_blocks_3  module_related_func          relu   \n","5        output        output               output        output   \n","\n","  Original type Quantized type  Changed  \n","0             x              x    False  \n","1   BatchNorm1d    BatchNorm1d    False  \n","2          ReLU           ReLU    False  \n","3        Linear  LinearInteger     True  \n","4          ReLU           ReLU    False  \n","5        output         output    False  "]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["from chop.passes.graph.transforms.quantize.summary import graph_iterator_compare_nodes\n","graph_iterator_compare_nodes(ori_mg, mg)"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'chop.passes.graph.transforms.quantize.quantized_modules.linear.LinearInteger'>\n","Difference found at name: seq_blocks_2, MASE type: module_related_func, MASE operation: linear\n","Original module: <class 'torch.nn.modules.linear.Linear'> --> New module: <class 'chop.passes.graph.transforms.quantize.quantized_modules.linear.LinearInteger'>\n","Precision of original module: [32]\n","Precision of modified module: [8, 4]\n","Weight of original module: Parameter containing:\n","tensor([[-2.0059e-01,  7.0937e-02, -9.0220e-01, -3.3770e-01, -2.1713e-01,\n","         -4.4070e-01, -1.0396e-02, -2.6671e-02,  1.2821e-01,  5.0434e-01,\n","          6.0154e-02, -9.6376e-02, -1.2634e-01, -5.2315e-02,  1.1104e-01,\n","         -1.4309e-02],\n","        [-5.8988e-02,  7.6669e-02,  1.1914e-01,  1.6312e-01, -3.9841e-01,\n","          1.9455e-01,  1.9242e-02,  2.9598e-01,  1.4485e-01, -1.7565e-01,\n","          1.7875e-01, -2.9895e-01,  6.2455e-02, -2.7527e-01, -9.8624e-02,\n","          6.7748e-02],\n","        [-6.6721e-02,  5.2548e-02, -4.9106e-01, -3.0251e-01,  3.0298e-01,\n","          1.7063e-01,  8.4487e-02,  6.0341e-02,  7.6235e-02, -1.6422e-01,\n","         -4.4930e-02, -2.4269e-01, -1.1802e-01, -2.9312e-01,  2.6968e-01,\n","         -1.1605e-02],\n","        [ 1.7765e-01,  1.4337e-04,  5.8556e-01,  2.9210e-01, -4.0488e-01,\n","          1.5749e-01, -5.0958e-02, -3.6898e-02, -3.4969e-02, -2.1817e-01,\n","         -2.5161e-02, -5.8310e-02,  2.1869e-01, -1.1095e-01, -5.9321e-01,\n","          4.2452e-05],\n","        [ 2.0397e-01, -8.7888e-03,  2.7631e-01, -1.0005e-01,  4.2933e-02,\n","          5.7423e-01, -7.8329e-03,  9.6181e-02,  1.3335e-01, -3.6843e-02,\n","          8.5629e-03,  2.1146e-01, -2.8945e-02, -1.8978e-01, -8.5525e-02,\n","          9.1451e-02],\n","        [ 5.9642e-02, -1.2030e-02, -2.2063e-01, -1.0121e+00,  3.8642e-02,\n","         -4.3222e-01, -9.6167e-03, -1.3463e-02, -2.1588e-02, -3.7499e-01,\n","         -5.3905e-02,  1.3014e-01,  1.2544e-02,  7.9407e-02, -1.6949e-01,\n","          3.7900e-02],\n","        [ 1.2626e-01,  1.2629e-01, -2.8521e-01,  1.7010e-01, -4.1231e-02,\n","          3.2585e-01, -7.2340e-02, -5.9621e-02, -1.3556e-02,  1.5605e-01,\n","         -3.6087e-03, -1.4235e-01,  1.7300e-02, -5.1551e-02, -9.2693e-01,\n","          8.1758e-02],\n","        [-3.8899e-02,  5.4048e-02, -2.4617e-01, -3.1919e-01,  3.9084e-01,\n","          2.8990e-01, -6.8768e-02,  1.2402e-02, -1.3238e-01, -2.0666e-02,\n","         -1.3440e-01,  1.5782e-01, -2.4401e-01, -1.8689e-01, -5.5418e-01,\n","          7.4639e-02],\n","        [ 4.6317e-01,  3.6642e-01, -2.4689e-02,  2.8054e-01,  8.3301e-03,\n","          4.9456e-02, -4.5266e-02,  5.3558e-03,  3.7972e-02, -1.4745e-01,\n","          1.9983e-01,  1.6850e-01, -1.5898e-01, -8.2602e-02,  1.7561e-01,\n","         -5.1536e-01],\n","        [-1.1896e-01,  5.4313e-02,  5.7660e-01, -2.2969e-02,  2.3570e-01,\n","         -5.4054e-01, -6.0586e-02, -2.1382e-01, -3.7689e-03,  1.4558e-01,\n","          1.4677e-01, -1.8231e-01,  6.6321e-03, -2.1710e-01,  4.2843e-01,\n","          2.3521e-01],\n","        [-4.7850e-02,  3.0811e-03,  2.8922e-02, -1.3289e-01,  7.5705e-01,\n","          7.0069e-01, -7.7228e-04, -3.9782e-02,  8.3352e-02, -1.2090e-01,\n","         -3.5210e-02, -7.3726e-02, -3.3544e-02,  5.8560e-02, -9.1811e-02,\n","          4.0592e-02],\n","        [-3.1566e-03,  7.7775e-02,  4.0415e-01,  2.9758e-01,  4.9813e-01,\n","          3.0583e-01,  5.2763e-02,  2.2709e-02, -5.2243e-02, -5.1048e-02,\n","         -5.0498e-03, -5.5277e-02, -5.7427e-02, -7.3168e-02, -1.0819e-01,\n","         -5.1632e-02],\n","        [-3.1210e-01,  6.6336e-02, -1.3284e-01, -4.6676e-01, -2.0605e-02,\n","          3.5000e-01,  3.6194e-02,  2.3955e-02, -6.7453e-03,  5.5884e-03,\n","         -8.1097e-03, -1.4790e-01, -2.0852e-01, -2.6397e-01, -4.5408e-02,\n","          4.6907e-01],\n","        [ 6.7421e-01, -1.7944e-01,  2.9643e-02,  3.7234e-01, -3.2145e-01,\n","         -1.6300e-01,  1.0448e-01,  9.5591e-03, -9.2681e-03,  5.9645e-02,\n","          7.1186e-02, -1.8952e-01, -1.1676e-02,  3.6753e-01,  1.5833e-01,\n","          2.7272e-01],\n","        [ 2.4580e-01,  1.0944e+00,  2.0972e-01, -1.6592e-01, -1.5506e-03,\n","          1.1730e-01,  3.3471e-02,  3.2407e-02, -2.8816e-03, -1.7589e-01,\n","         -2.0137e-02, -1.9553e-01,  1.5791e-01, -6.2062e-02, -9.0140e-02,\n","         -6.1251e-02],\n","        [ 1.0233e-01,  8.9854e-02,  1.5912e-01, -8.6085e-01, -2.6688e-01,\n","         -5.8660e-01, -4.7901e-02, -4.2384e-03, -1.9764e-02, -1.2603e-01,\n","          2.1983e-02,  5.9897e-02,  3.5478e-02, -4.2340e-02,  4.7219e-02,\n","         -3.2137e-02],\n","        [ 2.0020e-01,  1.6558e-01, -2.4486e-02, -1.5050e-01,  2.8481e-01,\n","          2.7242e-02,  3.7524e-02, -2.0719e-02,  1.1922e-01,  1.2118e-01,\n","         -5.9027e-02,  5.5232e-02,  7.7847e-04,  4.1753e-02,  1.9697e-01,\n","         -6.8511e-01],\n","        [-2.5680e-01, -4.0553e-02, -1.0061e+00, -4.0851e-01,  6.2976e-01,\n","         -1.0653e-01,  5.0339e-02, -9.1563e-02, -8.7936e-03, -1.4234e-01,\n","         -1.1332e-01,  2.3134e-01,  1.0171e-01,  1.8080e-01, -1.0018e-04,\n","         -2.4517e-01],\n","        [-1.9594e-03,  2.3613e-01, -7.5785e-03, -1.8781e-01,  4.8717e-01,\n","          4.2218e-01, -1.9048e-02, -5.8856e-02, -6.9094e-03, -4.4130e-01,\n","          7.4568e-03, -3.2101e-02, -6.9265e-02,  2.3161e-01, -3.8984e-01,\n","          1.4813e-01],\n","        [ 1.1139e-01,  2.2572e-01,  4.5279e-02,  4.6271e-01, -3.1994e-01,\n","         -2.5982e-01, -4.9823e-03,  6.7048e-02,  7.1614e-02, -6.8814e-02,\n","         -1.3279e-02,  1.6025e-01, -6.9438e-02,  2.4857e-01,  7.2282e-02,\n","         -3.4878e-01],\n","        [-1.0125e-01, -2.5074e-03,  4.2796e-03,  1.8319e-01, -1.9333e-01,\n","          4.7450e-01,  8.5678e-02, -7.8711e-02, -1.5341e-01, -7.7191e-01,\n","         -7.3733e-02, -1.2606e-01,  1.2681e-01, -7.1683e-02, -4.6067e-01,\n","          6.0593e-02],\n","        [ 2.0854e-01,  5.5756e-01,  1.0758e-01,  1.5331e-01, -3.8276e-01,\n","          6.9252e-01, -6.2024e-03,  5.8844e-02, -1.2077e-01,  1.0147e-01,\n","          1.2220e-01,  9.8703e-02, -6.0578e-02, -1.8373e-01, -9.4474e-02,\n","          4.4981e-01],\n","        [ 4.7199e-01,  2.3263e-01,  1.2653e-02,  1.2315e-02,  1.9865e-01,\n","         -4.8553e-02, -9.0715e-02, -9.2154e-02,  4.6556e-02,  3.0430e-01,\n","          1.4070e-02,  2.0461e-01, -3.7386e-02,  5.9263e-01, -4.9833e-02,\n","         -1.1189e-01],\n","        [-4.6865e-02,  4.3867e-02,  2.6694e-01,  9.5424e-02, -6.5953e-01,\n","         -4.7973e-01, -1.9556e-02, -3.5709e-03,  5.0422e-01,  2.3339e-01,\n","          8.3234e-02,  1.2063e-01,  1.6169e-01, -2.4385e-01, -1.3265e-01,\n","         -2.2362e-01],\n","        [ 2.0066e-01,  1.3273e-02, -9.6549e-02,  5.6670e-01, -1.4434e-01,\n","          5.4448e-02,  3.9324e-02, -3.6271e-02, -6.2628e-02, -3.0779e-01,\n","         -1.0597e-01,  1.2995e-01,  2.4835e-02, -1.9039e-01,  1.7803e-01,\n","          5.8692e-01],\n","        [-3.5334e-03,  1.5600e-01,  2.6022e-01, -3.8902e-01, -4.2761e-01,\n","         -1.0709e+00, -8.2554e-03,  4.1584e-02, -2.0003e-01, -1.0091e-03,\n","         -2.2462e-02, -4.9200e-02, -7.2636e-02, -1.0734e-01, -8.8298e-02,\n","         -5.1474e-02],\n","        [ 1.7638e-02, -1.4447e-01,  3.5353e-02,  5.7085e-01, -2.9861e-02,\n","         -3.9249e-01,  7.2390e-03, -1.2640e-04,  1.2426e-01, -8.6779e-02,\n","         -1.7023e-02, -2.0655e-01, -1.0530e-01,  2.0993e-01, -2.1015e-01,\n","         -2.9481e-01],\n","        [-2.0774e-01,  1.2644e-01, -2.9095e-01, -6.0485e-01,  1.5578e-01,\n","         -4.5455e-01,  6.2546e-03, -7.8750e-02,  8.1266e-02,  1.7562e-01,\n","         -7.3517e-02,  1.5760e-01, -2.3775e-01,  2.0150e-01,  1.6808e-01,\n","          7.4029e-04],\n","        [-7.0307e-02, -8.1635e-02,  5.4098e-02,  2.7457e-01, -2.8762e-01,\n","         -5.6760e-02, -6.7686e-02,  2.0108e-02,  5.4072e-02,  3.3201e-01,\n","         -5.9626e-03,  8.0956e-02, -1.3376e-01,  2.8403e-02, -7.0960e-01,\n","         -3.0078e-02],\n","        [-1.6936e-01, -1.4551e-01, -5.0098e-02, -5.5542e-01, -2.9871e-01,\n","         -2.8162e-01,  1.0106e-01,  1.6375e-01,  8.6139e-02,  4.1048e-01,\n","         -9.9064e-02, -3.0374e-01, -1.3352e-01,  4.4159e-01, -1.7307e-02,\n","         -8.4360e-02],\n","        [ 7.3594e-02, -1.1736e-01,  2.2532e-01,  2.5168e-01,  7.6203e-02,\n","          2.3858e-01,  6.5483e-02,  7.9023e-03, -3.1687e-02, -2.4319e-01,\n","         -1.3380e-02,  6.9842e-02,  1.9251e-01, -1.9482e-01,  5.4571e-01,\n","         -6.7600e-02],\n","        [-9.8188e-02, -6.5727e-02,  7.4846e-01,  1.9858e-01, -5.2381e-01,\n","         -4.9542e-01, -1.2697e-01, -6.3248e-02,  5.3741e-04,  1.0860e-01,\n","          2.0731e-02, -2.9215e-02, -2.7521e-01,  1.9574e-01,  1.3199e-02,\n","          6.6513e-02]], requires_grad=True)\n","Weight of quantized module: tensor([[-0.1875,  0.0625, -0.8750, -0.3125, -0.1875, -0.4375, -0.0000, -0.0000,\n","          0.1250,  0.5000,  0.0625, -0.1250, -0.1250, -0.0625,  0.1250, -0.0000],\n","        [-0.0625,  0.0625,  0.1250,  0.1875, -0.3750,  0.1875,  0.0000,  0.3125,\n","          0.1250, -0.1875,  0.1875, -0.3125,  0.0625, -0.2500, -0.1250,  0.0625],\n","        [-0.0625,  0.0625, -0.5000, -0.3125,  0.3125,  0.1875,  0.0625,  0.0625,\n","          0.0625, -0.1875, -0.0625, -0.2500, -0.1250, -0.3125,  0.2500, -0.0000],\n","        [ 0.1875,  0.0000,  0.5625,  0.3125, -0.3750,  0.1875, -0.0625, -0.0625,\n","         -0.0625, -0.1875, -0.0000, -0.0625,  0.1875, -0.1250, -0.5625,  0.0000],\n","        [ 0.1875, -0.0000,  0.2500, -0.1250,  0.0625,  0.5625, -0.0000,  0.1250,\n","          0.1250, -0.0625,  0.0000,  0.1875, -0.0000, -0.1875, -0.0625,  0.0625],\n","        [ 0.0625, -0.0000, -0.2500, -1.0000,  0.0625, -0.4375, -0.0000, -0.0000,\n","         -0.0000, -0.3750, -0.0625,  0.1250,  0.0000,  0.0625, -0.1875,  0.0625],\n","        [ 0.1250,  0.1250, -0.3125,  0.1875, -0.0625,  0.3125, -0.0625, -0.0625,\n","         -0.0000,  0.1250, -0.0000, -0.1250,  0.0000, -0.0625, -0.9375,  0.0625],\n","        [-0.0625,  0.0625, -0.2500, -0.3125,  0.3750,  0.3125, -0.0625,  0.0000,\n","         -0.1250, -0.0000, -0.1250,  0.1875, -0.2500, -0.1875, -0.5625,  0.0625],\n","        [ 0.4375,  0.3750, -0.0000,  0.2500,  0.0000,  0.0625, -0.0625,  0.0000,\n","          0.0625, -0.1250,  0.1875,  0.1875, -0.1875, -0.0625,  0.1875, -0.5000],\n","        [-0.1250,  0.0625,  0.5625, -0.0000,  0.2500, -0.5625, -0.0625, -0.1875,\n","         -0.0000,  0.1250,  0.1250, -0.1875,  0.0000, -0.1875,  0.4375,  0.2500],\n","        [-0.0625,  0.0000,  0.0000, -0.1250,  0.7500,  0.6875, -0.0000, -0.0625,\n","          0.0625, -0.1250, -0.0625, -0.0625, -0.0625,  0.0625, -0.0625,  0.0625],\n","        [-0.0000,  0.0625,  0.3750,  0.3125,  0.5000,  0.3125,  0.0625,  0.0000,\n","         -0.0625, -0.0625, -0.0000, -0.0625, -0.0625, -0.0625, -0.1250, -0.0625],\n","        [-0.3125,  0.0625, -0.1250, -0.4375, -0.0000,  0.3750,  0.0625,  0.0000,\n","         -0.0000,  0.0000, -0.0000, -0.1250, -0.1875, -0.2500, -0.0625,  0.5000],\n","        [ 0.6875, -0.1875,  0.0000,  0.3750, -0.3125, -0.1875,  0.1250,  0.0000,\n","         -0.0000,  0.0625,  0.0625, -0.1875, -0.0000,  0.3750,  0.1875,  0.2500],\n","        [ 0.2500,  1.1250,  0.1875, -0.1875, -0.0000,  0.1250,  0.0625,  0.0625,\n","         -0.0000, -0.1875, -0.0000, -0.1875,  0.1875, -0.0625, -0.0625, -0.0625],\n","        [ 0.1250,  0.0625,  0.1875, -0.8750, -0.2500, -0.5625, -0.0625, -0.0000,\n","         -0.0000, -0.1250,  0.0000,  0.0625,  0.0625, -0.0625,  0.0625, -0.0625],\n","        [ 0.1875,  0.1875, -0.0000, -0.1250,  0.3125,  0.0000,  0.0625, -0.0000,\n","          0.1250,  0.1250, -0.0625,  0.0625,  0.0000,  0.0625,  0.1875, -0.6875],\n","        [-0.2500, -0.0625, -1.0000, -0.4375,  0.6250, -0.1250,  0.0625, -0.0625,\n","         -0.0000, -0.1250, -0.1250,  0.2500,  0.1250,  0.1875, -0.0000, -0.2500],\n","        [-0.0000,  0.2500, -0.0000, -0.1875,  0.5000,  0.4375, -0.0000, -0.0625,\n","         -0.0000, -0.4375,  0.0000, -0.0625, -0.0625,  0.2500, -0.3750,  0.1250],\n","        [ 0.1250,  0.2500,  0.0625,  0.4375, -0.3125, -0.2500, -0.0000,  0.0625,\n","          0.0625, -0.0625, -0.0000,  0.1875, -0.0625,  0.2500,  0.0625, -0.3750],\n","        [-0.1250, -0.0000,  0.0000,  0.1875, -0.1875,  0.5000,  0.0625, -0.0625,\n","         -0.1250, -0.7500, -0.0625, -0.1250,  0.1250, -0.0625, -0.4375,  0.0625],\n","        [ 0.1875,  0.5625,  0.1250,  0.1250, -0.3750,  0.6875, -0.0000,  0.0625,\n","         -0.1250,  0.1250,  0.1250,  0.1250, -0.0625, -0.1875, -0.1250,  0.4375],\n","        [ 0.5000,  0.2500,  0.0000,  0.0000,  0.1875, -0.0625, -0.0625, -0.0625,\n","          0.0625,  0.3125,  0.0000,  0.1875, -0.0625,  0.5625, -0.0625, -0.1250],\n","        [-0.0625,  0.0625,  0.2500,  0.1250, -0.6875, -0.5000, -0.0000, -0.0000,\n","          0.5000,  0.2500,  0.0625,  0.1250,  0.1875, -0.2500, -0.1250, -0.2500],\n","        [ 0.1875,  0.0000, -0.1250,  0.5625, -0.1250,  0.0625,  0.0625, -0.0625,\n","         -0.0625, -0.3125, -0.1250,  0.1250,  0.0000, -0.1875,  0.1875,  0.5625],\n","        [-0.0000,  0.1250,  0.2500, -0.3750, -0.4375, -1.0625, -0.0000,  0.0625,\n","         -0.1875, -0.0000, -0.0000, -0.0625, -0.0625, -0.1250, -0.0625, -0.0625],\n","        [ 0.0000, -0.1250,  0.0625,  0.5625, -0.0000, -0.3750,  0.0000, -0.0000,\n","          0.1250, -0.0625, -0.0000, -0.1875, -0.1250,  0.1875, -0.1875, -0.3125],\n","        [-0.1875,  0.1250, -0.3125, -0.6250,  0.1250, -0.4375,  0.0000, -0.0625,\n","          0.0625,  0.1875, -0.0625,  0.1875, -0.2500,  0.1875,  0.1875,  0.0000],\n","        [-0.0625, -0.0625,  0.0625,  0.2500, -0.3125, -0.0625, -0.0625,  0.0000,\n","          0.0625,  0.3125, -0.0000,  0.0625, -0.1250,  0.0000, -0.6875, -0.0000],\n","        [-0.1875, -0.1250, -0.0625, -0.5625, -0.3125, -0.3125,  0.1250,  0.1875,\n","          0.0625,  0.4375, -0.1250, -0.3125, -0.1250,  0.4375, -0.0000, -0.0625],\n","        [ 0.0625, -0.1250,  0.2500,  0.2500,  0.0625,  0.2500,  0.0625,  0.0000,\n","         -0.0625, -0.2500, -0.0000,  0.0625,  0.1875, -0.1875,  0.5625, -0.0625],\n","        [-0.1250, -0.0625,  0.7500,  0.1875, -0.5000, -0.5000, -0.1250, -0.0625,\n","          0.0000,  0.1250,  0.0000, -0.0000, -0.2500,  0.1875,  0.0000,  0.0625]],\n","       grad_fn=<IntegerQuantizeBackward>)\n","Random generated test input: tensor([ 1.0898, -0.8687, -0.4327, -1.5950, -1.2319, -0.2753,  0.9971, -1.5572,\n","         1.4411,  1.6771,  1.1327,  0.5432, -0.3454,  0.9762,  0.1316,  0.4610])\n","Output for original module: tensor([ 2.1402, -0.9189, -0.2849, -0.6000, -0.0317,  1.2634,  0.0107, -0.4655,\n","        -0.3492, -0.1454, -0.8291, -1.9391,  0.2321,  1.2505, -0.8882,  1.5699,\n","         0.1270, -0.1082, -0.8347, -0.1446, -2.0477, -0.2854,  1.1121,  1.4202,\n","        -1.0155,  0.5712, -0.7312,  1.5555,  0.3428,  2.1329, -1.1495,  0.4423],\n","       grad_fn=<ViewBackward0>)\n","Output for quantized module: tensor([ 2.0039, -1.0352, -0.4141, -0.6172, -0.1719,  1.2656,  0.0039, -0.4023,\n","        -0.2773, -0.2773, -0.8086, -1.9297,  0.2461,  1.3008, -0.9023,  1.5273,\n","         0.0508, -0.0195, -0.8555, -0.1367, -2.0156, -0.2227,  1.1250,  1.3867,\n","        -1.0273,  0.6016, -0.7578,  1.6406,  0.3867,  2.1211, -1.1719,  0.4062],\n","       grad_fn=<ViewBackward0>)\n","<class 'chop.passes.graph.transforms.quantize.quantized_modules.linear.LinearInteger'>\n","Difference found at name: seq_blocks_5, MASE type: module_related_func, MASE operation: linear\n","Original module: <class 'torch.nn.modules.linear.Linear'> --> New module: <class 'chop.passes.graph.transforms.quantize.quantized_modules.linear.LinearInteger'>\n","Precision of original module: [32]\n","Precision of modified module: [8, 4]\n","Weight of original module: Parameter containing:\n","tensor([[-9.3865e-03, -4.4277e-02, -8.8426e-02,  1.8823e-01, -9.4592e-02,\n","          3.5020e-02,  2.1780e-01, -2.4669e-03,  4.5797e-01,  2.8514e-02,\n","          2.0174e-01,  2.3578e-01, -5.1667e-02, -1.0595e-01,  6.3118e-01,\n","         -4.6962e-02,  4.3133e-01,  6.3068e-02,  2.6001e-01,  3.0305e-01,\n","         -3.3856e-02, -9.7126e-02,  3.2829e-01,  1.1802e-01, -2.5944e-01,\n","         -8.8591e-02,  1.1392e-01, -1.6266e-01, -6.2965e-02, -9.8105e-02,\n","          1.8416e-01,  4.7282e-02],\n","        [ 2.0575e-01, -1.1983e-01,  1.4202e-01, -1.3424e-01,  1.4884e-02,\n","         -1.8929e-01,  2.3364e-02,  8.4770e-02,  8.2159e-02, -3.4340e-01,\n","          6.7766e-01,  1.5137e-01, -4.6183e-04, -7.3303e-02,  1.4035e-01,\n","         -3.9131e-01, -2.2095e-02,  4.9189e-03,  3.3001e-01, -7.0730e-02,\n","         -2.9368e-01,  3.8562e-02,  1.7754e-01,  1.8061e-01, -2.0838e-01,\n","         -1.3642e-02, -1.9942e-01, -1.0518e-01, -1.1185e-01, -1.8169e-01,\n","         -8.8553e-02, -3.2384e-01],\n","        [-4.6750e-01, -9.8992e-02, -4.4565e-01, -1.4682e-01,  8.7814e-02,\n","         -1.2514e-01, -5.0218e-01, -9.7685e-03, -1.7838e-01, -2.8225e-01,\n","          2.8413e-01,  4.3216e-01, -2.3323e-03, -2.6992e-01, -3.9600e-03,\n","         -8.7342e-02, -1.7323e-01, -7.4453e-02,  1.1645e-01,  1.0299e-01,\n","         -7.5567e-02,  5.3015e-02,  6.3292e-03,  7.8200e-02, -1.4271e-01,\n","          1.5661e-01,  1.8347e-01, -8.4025e-02, -2.7845e-01, -1.6046e-01,\n","         -5.3043e-01,  6.4706e-02],\n","        [ 1.9075e-03,  1.7508e-01, -2.1856e-01, -5.6208e-02,  9.4070e-02,\n","          8.0784e-02,  5.2537e-02,  4.0826e-01, -2.9825e-01, -6.4410e-02,\n","         -1.5905e-02,  1.3620e-01,  2.9638e-01, -1.6196e-01,  1.6955e-01,\n","          3.0595e-02, -2.0349e-01, -3.2091e-02,  2.1295e-01, -9.3390e-02,\n","         -1.6564e-01,  3.0578e-01, -2.9738e-02, -7.8846e-02, -3.1073e-01,\n","         -5.7953e-02, -2.2082e-01,  1.9189e-01, -3.8443e-01, -1.5066e-01,\n","         -3.1944e-01, -1.5898e-01],\n","        [ 1.8773e-01, -4.6258e-03,  2.2979e-01,  2.6003e-01, -1.4713e-01,\n","          6.3969e-02,  2.1206e-01, -3.0467e-01,  7.1338e-02, -1.5588e-01,\n","          3.7697e-02,  5.2116e-02,  3.8525e-02, -1.3840e-01, -2.1112e-01,\n","         -1.6042e-01, -1.2093e-01,  1.2566e-02, -1.5365e-01, -2.7011e-01,\n","          2.4980e-01, -3.0707e-02, -1.7362e-01,  2.4047e-02, -2.1174e-03,\n","          3.4720e-01, -8.9778e-02,  2.3231e-02,  2.3471e-01, -4.4224e-01,\n","          7.5950e-03,  3.0610e-01],\n","        [-3.4325e-03,  3.2792e-03,  3.2954e-01, -2.3291e-01, -1.6128e-01,\n","          1.1724e-01,  8.0590e-02,  2.1623e-02, -4.5424e-02,  1.9170e-01,\n","         -5.5544e-01, -6.5342e-01,  1.7312e-01,  6.6225e-02, -2.1965e-02,\n","          1.8460e-01,  7.7343e-03,  1.9945e-02, -4.2701e-01, -2.4864e-01,\n","          4.1685e-02,  1.1294e-01, -6.7226e-02, -2.4260e-01,  2.3376e-01,\n","         -1.6879e-01, -2.7306e-01,  2.1063e-01,  2.1846e-02, -7.1037e-03,\n","          8.2314e-02, -2.6599e-01],\n","        [ 4.4311e-01,  2.8163e-02,  2.0818e-02, -3.0134e-01, -1.7776e-01,\n","         -9.1409e-03,  4.2316e-01, -1.9040e-01, -2.2178e-02,  1.8846e-01,\n","         -3.9496e-01, -6.2608e-01,  2.9277e-02, -4.7100e-02, -1.1189e-02,\n","          2.5146e-01,  5.5256e-02,  1.9126e-01, -2.6555e-01, -6.9054e-03,\n","          2.8238e-01,  5.7195e-02,  1.7744e-02, -1.3345e-01,  7.5751e-02,\n","         -5.1866e-03, -1.4483e-01,  1.6200e-01,  3.1059e-01,  3.2434e-01,\n","          2.1018e-02, -1.5125e-01],\n","        [-5.9292e-01, -2.2683e-01, -8.2936e-03,  2.4248e-01, -7.1847e-02,\n","         -6.8204e-02, -1.5728e-01,  2.6345e-01,  1.1583e-01,  4.7231e-02,\n","          1.0966e-01,  5.0778e-01, -1.0709e-01,  1.0782e-01,  4.9616e-02,\n","         -1.1671e-01,  4.8302e-02,  1.8871e-01,  6.9311e-03, -3.5337e-01,\n","          8.5875e-02, -3.2340e-01, -1.0179e-01,  1.2651e-01,  7.2844e-03,\n","         -8.5312e-02, -1.9999e-01, -2.3360e-01, -6.6991e-01, -2.7498e-01,\n","          6.2444e-02,  8.2281e-02],\n","        [ 1.3014e-01, -1.6110e-01,  3.7512e-02, -4.9988e-01, -2.4832e-01,\n","          3.4696e-01, -2.6105e-01, -9.1796e-02,  8.1821e-02,  6.5561e-02,\n","         -1.2247e-01, -2.4202e-01, -1.5242e-01,  3.6289e-03, -3.3293e-01,\n","          1.3575e-01,  1.5754e-01,  1.4009e-01, -4.2982e-01,  1.5967e-01,\n","          1.9082e-01, -2.9726e-01,  3.8194e-02, -5.9388e-02, -5.1681e-02,\n","          6.5240e-02,  1.0619e-01,  3.8306e-02, -1.5374e-01,  1.8831e-01,\n","          5.7314e-02, -6.5691e-02],\n","        [ 1.8097e-01,  1.6158e-01, -1.3099e-01,  7.7926e-02, -9.4715e-02,\n","         -1.4254e-01,  8.8650e-02,  1.1416e-01, -7.0758e-02,  2.0139e-01,\n","         -1.3555e-01,  1.9767e-01, -1.7834e-01,  9.5368e-03,  3.3749e-02,\n","          1.1740e-01, -1.2242e-01,  1.7881e-01, -7.1995e-02, -5.0330e-02,\n","          2.1845e-01,  4.0676e-02,  4.2460e-02, -1.6356e-01,  1.7357e-01,\n","          3.8613e-01,  2.9488e-02,  2.7349e-02,  5.3098e-01, -5.2642e-01,\n","          1.4933e-01,  3.0122e-01],\n","        [-2.0499e-01, -1.0978e-01,  2.8694e-02, -3.0228e-01,  8.4898e-03,\n","          5.2752e-02, -1.6422e-01, -7.2786e-01, -1.5077e-01,  7.1752e-02,\n","          2.6656e-01,  3.0609e-01,  8.2773e-02,  3.2811e-01,  6.4666e-02,\n","          2.6378e-01, -2.9351e-02, -1.1415e-01, -1.4518e-01,  1.5335e-01,\n","         -2.8010e-01,  2.1296e-01, -1.1282e-01, -9.1093e-03,  9.5821e-02,\n","          2.3382e-01,  1.9382e-02, -5.6365e-02, -2.0620e-01, -1.7380e-01,\n","          1.1800e-01, -3.6228e-02],\n","        [-2.7611e-01,  1.3692e-01,  1.8718e-01,  2.6855e-01,  1.9197e-01,\n","         -1.9468e-01,  9.0942e-01, -1.3441e-01, -6.1796e-02, -7.8045e-02,\n","         -1.1242e-01,  3.2650e-02,  9.2857e-02,  6.9577e-02,  2.8708e-01,\n","         -6.3972e-02,  4.8090e-02, -3.4787e-01,  2.9069e-01,  1.6052e-02,\n","         -7.0486e-02,  1.6921e-01, -3.8358e-02, -1.0990e-01,  1.4707e-02,\n","         -2.0007e-01, -2.5576e-01,  7.0879e-02,  3.4355e-01, -5.0814e-02,\n","          4.8319e-02, -2.1685e-01],\n","        [ 4.2539e-01,  4.8452e-02, -1.0208e-01, -3.5201e-01,  8.2046e-02,\n","          3.6513e-01,  1.0789e-01, -1.5835e-01, -3.0224e-02,  5.6889e-02,\n","          6.2310e-02,  3.0833e-01, -4.4278e-01, -2.0379e-01,  3.9979e-02,\n","          2.5806e-01,  1.8289e-01, -1.9933e-02, -3.1803e-01, -2.2957e-01,\n","          1.3553e-01, -4.5036e-02, -2.7182e-01, -3.9562e-02, -2.9703e-01,\n","          3.5550e-01, -5.4582e-02, -6.1746e-03, -3.6585e-02, -2.0899e-01,\n","          1.1388e-01,  8.1310e-02],\n","        [ 1.5373e-01, -2.2222e-01,  1.1998e-01, -4.6373e-01, -2.8989e-01,\n","          2.3742e-01, -2.6151e-01,  3.6157e-03,  1.1899e-01,  1.0470e-01,\n","         -6.8698e-02, -2.2304e-03, -8.4089e-02, -4.0621e-01, -1.3850e-01,\n","          1.2228e-01,  3.1920e-01,  1.8295e-01,  1.0883e-03, -6.0665e-02,\n","          5.1999e-03, -2.0060e-01,  3.4556e-01, -2.3749e-01, -4.0461e-01,\n","         -9.9985e-02, -1.8850e-02,  1.4688e-01, -2.5601e-01,  1.2115e-01,\n","         -2.3375e-01, -1.6331e-01],\n","        [-4.6662e-01,  1.4004e-01,  1.9885e-01, -7.8071e-02,  1.2727e-01,\n","          4.4079e-02, -1.3995e-01,  1.5461e-01,  2.0460e-01, -1.4791e-02,\n","         -4.9493e-02, -6.7903e-02,  1.2199e-01,  2.3286e-01,  1.4072e-01,\n","         -6.3967e-02,  8.6811e-02, -2.0062e-01,  3.3139e-01, -2.6665e-01,\n","         -1.9718e-01,  4.1145e-02, -5.5247e-02,  2.9052e-01,  7.7129e-02,\n","         -2.9981e-01, -3.1620e-01,  8.8812e-02, -5.0103e-01,  7.2377e-02,\n","         -1.9732e-01, -1.1956e-02],\n","        [ 2.3934e-01, -1.3187e-01,  8.7947e-02, -6.4035e-02, -2.5076e-01,\n","          2.1198e-01,  8.5156e-02,  3.1132e-01,  6.4338e-02,  1.3417e-01,\n","         -3.1993e-02, -3.9797e-01,  1.3521e-02, -6.1146e-02, -1.0846e-01,\n","         -9.3476e-02, -1.0295e-01,  4.0461e-01,  9.5553e-02, -2.2581e-01,\n","         -8.3627e-02, -8.0458e-02,  8.0132e-02, -4.4833e-02, -1.1699e-01,\n","         -1.2352e-01, -5.0132e-01,  1.8802e-01, -1.3038e-01,  1.7699e-01,\n","         -2.3401e-01, -1.9644e-01]], requires_grad=True)\n","Weight of quantized module: tensor([[-0.0000, -0.0625, -0.0625,  0.1875, -0.1250,  0.0625,  0.1875, -0.0000,\n","          0.4375,  0.0000,  0.1875,  0.2500, -0.0625, -0.1250,  0.6250, -0.0625,\n","          0.4375,  0.0625,  0.2500,  0.3125, -0.0625, -0.1250,  0.3125,  0.1250,\n","         -0.2500, -0.0625,  0.1250, -0.1875, -0.0625, -0.1250,  0.1875,  0.0625],\n","        [ 0.1875, -0.1250,  0.1250, -0.1250,  0.0000, -0.1875,  0.0000,  0.0625,\n","          0.0625, -0.3125,  0.6875,  0.1250, -0.0000, -0.0625,  0.1250, -0.3750,\n","         -0.0000,  0.0000,  0.3125, -0.0625, -0.3125,  0.0625,  0.1875,  0.1875,\n","         -0.1875, -0.0000, -0.1875, -0.1250, -0.1250, -0.1875, -0.0625, -0.3125],\n","        [-0.4375, -0.1250, -0.4375, -0.1250,  0.0625, -0.1250, -0.5000, -0.0000,\n","         -0.1875, -0.3125,  0.3125,  0.4375, -0.0000, -0.2500, -0.0000, -0.0625,\n","         -0.1875, -0.0625,  0.1250,  0.1250, -0.0625,  0.0625,  0.0000,  0.0625,\n","         -0.1250,  0.1875,  0.1875, -0.0625, -0.2500, -0.1875, -0.5000,  0.0625],\n","        [ 0.0000,  0.1875, -0.1875, -0.0625,  0.1250,  0.0625,  0.0625,  0.4375,\n","         -0.3125, -0.0625, -0.0000,  0.1250,  0.3125, -0.1875,  0.1875,  0.0000,\n","         -0.1875, -0.0625,  0.1875, -0.0625, -0.1875,  0.3125, -0.0000, -0.0625,\n","         -0.3125, -0.0625, -0.2500,  0.1875, -0.3750, -0.1250, -0.3125, -0.1875],\n","        [ 0.1875, -0.0000,  0.2500,  0.2500, -0.1250,  0.0625,  0.1875, -0.3125,\n","          0.0625, -0.1250,  0.0625,  0.0625,  0.0625, -0.1250, -0.1875, -0.1875,\n","         -0.1250,  0.0000, -0.1250, -0.2500,  0.2500, -0.0000, -0.1875,  0.0000,\n","         -0.0000,  0.3750, -0.0625,  0.0000,  0.2500, -0.4375,  0.0000,  0.3125],\n","        [-0.0000,  0.0000,  0.3125, -0.2500, -0.1875,  0.1250,  0.0625,  0.0000,\n","         -0.0625,  0.1875, -0.5625, -0.6250,  0.1875,  0.0625, -0.0000,  0.1875,\n","          0.0000,  0.0000, -0.4375, -0.2500,  0.0625,  0.1250, -0.0625, -0.2500,\n","          0.2500, -0.1875, -0.2500,  0.1875,  0.0000, -0.0000,  0.0625, -0.2500],\n","        [ 0.4375,  0.0000,  0.0000, -0.3125, -0.1875, -0.0000,  0.4375, -0.1875,\n","         -0.0000,  0.1875, -0.3750, -0.6250,  0.0000, -0.0625, -0.0000,  0.2500,\n","          0.0625,  0.1875, -0.2500, -0.0000,  0.3125,  0.0625,  0.0000, -0.1250,\n","          0.0625, -0.0000, -0.1250,  0.1875,  0.3125,  0.3125,  0.0000, -0.1250],\n","        [-0.5625, -0.2500, -0.0000,  0.2500, -0.0625, -0.0625, -0.1875,  0.2500,\n","          0.1250,  0.0625,  0.1250,  0.5000, -0.1250,  0.1250,  0.0625, -0.1250,\n","          0.0625,  0.1875,  0.0000, -0.3750,  0.0625, -0.3125, -0.1250,  0.1250,\n","          0.0000, -0.0625, -0.1875, -0.2500, -0.6875, -0.2500,  0.0625,  0.0625],\n","        [ 0.1250, -0.1875,  0.0625, -0.5000, -0.2500,  0.3750, -0.2500, -0.0625,\n","          0.0625,  0.0625, -0.1250, -0.2500, -0.1250,  0.0000, -0.3125,  0.1250,\n","          0.1875,  0.1250, -0.4375,  0.1875,  0.1875, -0.3125,  0.0625, -0.0625,\n","         -0.0625,  0.0625,  0.1250,  0.0625, -0.1250,  0.1875,  0.0625, -0.0625],\n","        [ 0.1875,  0.1875, -0.1250,  0.0625, -0.1250, -0.1250,  0.0625,  0.1250,\n","         -0.0625,  0.1875, -0.1250,  0.1875, -0.1875,  0.0000,  0.0625,  0.1250,\n","         -0.1250,  0.1875, -0.0625, -0.0625,  0.1875,  0.0625,  0.0625, -0.1875,\n","          0.1875,  0.3750,  0.0000,  0.0000,  0.5000, -0.5000,  0.1250,  0.3125],\n","        [-0.1875, -0.1250,  0.0000, -0.3125,  0.0000,  0.0625, -0.1875, -0.7500,\n","         -0.1250,  0.0625,  0.2500,  0.3125,  0.0625,  0.3125,  0.0625,  0.2500,\n","         -0.0000, -0.1250, -0.1250,  0.1250, -0.2500,  0.1875, -0.1250, -0.0000,\n","          0.1250,  0.2500,  0.0000, -0.0625, -0.1875, -0.1875,  0.1250, -0.0625],\n","        [-0.2500,  0.1250,  0.1875,  0.2500,  0.1875, -0.1875,  0.9375, -0.1250,\n","         -0.0625, -0.0625, -0.1250,  0.0625,  0.0625,  0.0625,  0.3125, -0.0625,\n","          0.0625, -0.3750,  0.3125,  0.0000, -0.0625,  0.1875, -0.0625, -0.1250,\n","          0.0000, -0.1875, -0.2500,  0.0625,  0.3125, -0.0625,  0.0625, -0.1875],\n","        [ 0.4375,  0.0625, -0.1250, -0.3750,  0.0625,  0.3750,  0.1250, -0.1875,\n","         -0.0000,  0.0625,  0.0625,  0.3125, -0.4375, -0.1875,  0.0625,  0.2500,\n","          0.1875, -0.0000, -0.3125, -0.2500,  0.1250, -0.0625, -0.2500, -0.0625,\n","         -0.3125,  0.3750, -0.0625, -0.0000, -0.0625, -0.1875,  0.1250,  0.0625],\n","        [ 0.1250, -0.2500,  0.1250, -0.4375, -0.3125,  0.2500, -0.2500,  0.0000,\n","          0.1250,  0.1250, -0.0625, -0.0000, -0.0625, -0.3750, -0.1250,  0.1250,\n","          0.3125,  0.1875,  0.0000, -0.0625,  0.0000, -0.1875,  0.3750, -0.2500,\n","         -0.3750, -0.1250, -0.0000,  0.1250, -0.2500,  0.1250, -0.2500, -0.1875],\n","        [-0.4375,  0.1250,  0.1875, -0.0625,  0.1250,  0.0625, -0.1250,  0.1250,\n","          0.1875, -0.0000, -0.0625, -0.0625,  0.1250,  0.2500,  0.1250, -0.0625,\n","          0.0625, -0.1875,  0.3125, -0.2500, -0.1875,  0.0625, -0.0625,  0.3125,\n","          0.0625, -0.3125, -0.3125,  0.0625, -0.5000,  0.0625, -0.1875, -0.0000],\n","        [ 0.2500, -0.1250,  0.0625, -0.0625, -0.2500,  0.1875,  0.0625,  0.3125,\n","          0.0625,  0.1250, -0.0625, -0.3750,  0.0000, -0.0625, -0.1250, -0.0625,\n","         -0.1250,  0.3750,  0.1250, -0.2500, -0.0625, -0.0625,  0.0625, -0.0625,\n","         -0.1250, -0.1250, -0.5000,  0.1875, -0.1250,  0.1875, -0.2500, -0.1875]],\n","       grad_fn=<IntegerQuantizeBackward>)\n","Random generated test input: tensor([-1.2561, -1.2985,  0.3747, -0.2857,  1.1408, -1.0817, -1.2873, -1.0976,\n","        -0.0883, -0.6262, -0.0297,  1.1913, -0.7042, -0.1936, -0.7958,  0.1714,\n","         0.9230,  1.0869, -0.9904,  0.0024,  1.5233, -1.4758,  1.1614,  0.1206,\n","        -0.4646, -0.5282,  1.7625, -0.3160, -0.2259,  1.9003,  0.3493, -0.5132])\n","Output for original module: tensor([ 0.4813, -0.6054,  1.6810, -2.5898, -1.2252, -1.2490, -0.6167,  1.6468,\n","         2.0823, -1.5944,  0.4733, -2.2935, -0.3237,  1.9392, -0.7355, -1.6805],\n","       grad_fn=<ViewBackward0>)\n","Output for quantized module: tensor([ 0.4961, -0.5742,  1.5859, -2.6641, -1.2500, -1.1914, -0.6289,  1.7383,\n","         2.1445, -1.7500,  0.6172, -2.4883, -0.3164,  2.0156, -0.7656, -1.7188],\n","       grad_fn=<ViewBackward0>)\n","<class 'chop.passes.graph.transforms.quantize.quantized_modules.linear.LinearInteger'>\n","Difference found at name: seq_blocks_8, MASE type: module_related_func, MASE operation: linear\n","Original module: <class 'torch.nn.modules.linear.Linear'> --> New module: <class 'chop.passes.graph.transforms.quantize.quantized_modules.linear.LinearInteger'>\n","Precision of original module: [32]\n","Precision of modified module: [8, 4]\n","Weight of original module: Parameter containing:\n","tensor([[ 2.8503e-01, -2.4187e-01,  2.9632e-01, -2.3284e-01, -9.7513e-02,\n","         -3.4525e-01,  1.5537e-01,  6.0373e-01, -5.3559e-02, -5.6506e-02,\n","          1.7504e-01,  6.7563e-02,  1.4649e-01, -8.2523e-02,  1.6492e-01,\n","         -7.7725e-02],\n","        [-6.6299e-02, -1.8386e-01,  1.4257e-01,  1.7533e-01, -2.6672e-01,\n","          4.0419e-02, -4.6937e-01,  3.2683e-01,  1.4349e-01, -3.1104e-01,\n","         -7.6031e-02, -2.6594e-01, -1.9770e-01, -1.2898e-04,  4.3715e-01,\n","          9.2998e-03],\n","        [-3.2757e-01, -1.0854e-01, -2.9945e-01,  3.1590e-01,  1.7741e-01,\n","         -7.1081e-02, -1.1005e-01, -3.2033e-01, -2.7057e-01,  1.2269e-01,\n","         -5.1512e-01, -1.7390e-01,  1.6326e-01, -4.7843e-03,  1.1504e-01,\n","          1.0025e-01],\n","        [-1.0666e-01,  9.3104e-03, -5.7052e-01,  2.1438e-01, -1.5043e-01,\n","         -4.7999e-02, -2.4174e-01,  5.1141e-02, -3.3691e-02,  6.6162e-02,\n","         -2.5410e-01, -7.9331e-02,  4.1718e-02, -1.1652e-03, -1.3933e-01,\n","         -1.2057e-01],\n","        [-1.3861e-01,  1.7030e-01, -2.7611e-03, -1.9987e-01, -9.3793e-02,\n","         -5.0584e-03,  2.2270e-01,  7.9652e-02,  1.0396e-01, -4.9225e-01,\n","          4.4972e-01, -1.2125e-01,  2.2268e-02, -3.3787e-01,  1.4471e-01,\n","          1.7229e-01],\n","        [-2.2287e-01, -4.9355e-01,  2.3441e-01, -2.8239e-01,  1.3962e-01,\n","         -8.0488e-02,  1.1470e-02, -2.0332e-02, -4.4618e-01,  9.8612e-02,\n","          6.9981e-02,  1.9970e-01, -3.3735e-01, -5.0068e-01,  4.3883e-02,\n","         -2.0797e-01],\n","        [-3.5550e-02, -3.1992e-03, -2.0386e-01,  4.2311e-01, -3.1807e-02,\n","         -3.3169e-01, -3.2983e-01, -6.6052e-02, -4.5152e-02,  1.2183e-02,\n","         -1.1786e-01, -2.4173e-01,  1.4775e-01, -1.6786e-01,  1.1705e-01,\n","         -1.7340e-01],\n","        [-5.4315e-02, -1.5970e-01,  1.5990e-01,  3.2229e-01, -1.5139e-01,\n","         -2.7702e-01,  2.1649e-02, -1.4650e-01, -3.8675e-01,  1.2521e-01,\n","          1.4858e-01, -4.0223e-01,  1.6205e-01,  5.1013e-02,  3.5823e-01,\n","         -3.1633e-01]], requires_grad=True)\n","Weight of quantized module: tensor([[ 0.3125, -0.2500,  0.3125, -0.2500, -0.1250, -0.3750,  0.1250,  0.6250,\n","         -0.0625, -0.0625,  0.1875,  0.0625,  0.1250, -0.0625,  0.1875, -0.0625],\n","        [-0.0625, -0.1875,  0.1250,  0.1875, -0.2500,  0.0625, -0.5000,  0.3125,\n","          0.1250, -0.3125, -0.0625, -0.2500, -0.1875, -0.0000,  0.4375,  0.0000],\n","        [-0.3125, -0.1250, -0.3125,  0.3125,  0.1875, -0.0625, -0.1250, -0.3125,\n","         -0.2500,  0.1250, -0.5000, -0.1875,  0.1875, -0.0000,  0.1250,  0.1250],\n","        [-0.1250,  0.0000, -0.5625,  0.1875, -0.1250, -0.0625, -0.2500,  0.0625,\n","         -0.0625,  0.0625, -0.2500, -0.0625,  0.0625, -0.0000, -0.1250, -0.1250],\n","        [-0.1250,  0.1875, -0.0000, -0.1875, -0.1250, -0.0000,  0.2500,  0.0625,\n","          0.1250, -0.5000,  0.4375, -0.1250,  0.0000, -0.3125,  0.1250,  0.1875],\n","        [-0.2500, -0.5000,  0.2500, -0.3125,  0.1250, -0.0625,  0.0000, -0.0000,\n","         -0.4375,  0.1250,  0.0625,  0.1875, -0.3125, -0.5000,  0.0625, -0.1875],\n","        [-0.0625, -0.0000, -0.1875,  0.4375, -0.0625, -0.3125, -0.3125, -0.0625,\n","         -0.0625,  0.0000, -0.1250, -0.2500,  0.1250, -0.1875,  0.1250, -0.1875],\n","        [-0.0625, -0.1875,  0.1875,  0.3125, -0.1250, -0.2500,  0.0000, -0.1250,\n","         -0.3750,  0.1250,  0.1250, -0.3750,  0.1875,  0.0625,  0.3750, -0.3125]],\n","       grad_fn=<IntegerQuantizeBackward>)\n","Random generated test input: tensor([-2.3893, -1.3301, -1.0956, -0.7570,  1.4044,  0.9023, -0.7094,  1.1108,\n","        -1.9314, -0.2375,  1.4090, -1.2595, -0.6651, -0.9827,  0.0921,  0.6917])\n","Output for original module: tensor([-0.1015,  0.8718,  1.0592,  0.2310,  0.9794,  2.4669,  0.0032,  0.5678],\n","       grad_fn=<ViewBackward0>)\n","Output for quantized module: tensor([-0.1250,  0.9219,  1.0820,  0.3672,  0.7852,  2.5195,  0.0508,  0.5664],\n","       grad_fn=<ViewBackward0>)\n","<class 'chop.passes.graph.transforms.quantize.quantized_modules.linear.LinearInteger'>\n","Difference found at name: seq_blocks_11, MASE type: module_related_func, MASE operation: linear\n","Original module: <class 'torch.nn.modules.linear.Linear'> --> New module: <class 'chop.passes.graph.transforms.quantize.quantized_modules.linear.LinearInteger'>\n","Precision of original module: [32]\n","Precision of modified module: [8, 4]\n","Weight of original module: Parameter containing:\n","tensor([[ 0.2618, -0.5175, -0.3254,  0.0716,  0.1549, -0.0914, -0.0130,  0.0812],\n","        [-0.0635, -0.2807, -0.1034,  0.2355,  0.4414,  0.1661, -0.1936,  0.0377],\n","        [-0.0527,  0.1949,  0.1404, -0.5508,  0.1216, -0.0447,  0.3747, -0.1527],\n","        [-0.0363,  0.0913,  0.0132, -0.1101,  0.1022, -0.0063,  0.4753, -0.5509],\n","        [-0.2986, -0.4857,  0.3317,  0.0886, -0.2743, -0.4919, -0.1644,  0.2120]],\n","       requires_grad=True)\n","Weight of quantized module: tensor([[ 0.2500, -0.5000, -0.3125,  0.0625,  0.1250, -0.0625, -0.0000,  0.0625],\n","        [-0.0625, -0.2500, -0.1250,  0.2500,  0.4375,  0.1875, -0.1875,  0.0625],\n","        [-0.0625,  0.1875,  0.1250, -0.5625,  0.1250, -0.0625,  0.3750, -0.1250],\n","        [-0.0625,  0.0625,  0.0000, -0.1250,  0.1250, -0.0000,  0.5000, -0.5625],\n","        [-0.3125, -0.5000,  0.3125,  0.0625, -0.2500, -0.5000, -0.1875,  0.1875]],\n","       grad_fn=<IntegerQuantizeBackward>)\n","Random generated test input: tensor([-0.3341, -0.4662, -0.8798,  1.2951, -1.2968,  1.1491,  0.3841,  0.3511])\n","Output for original module: tensor([ 0.4775,  0.1352, -0.7997, -0.1789,  0.1286], grad_fn=<ViewBackward0>)\n","Output for quantized module: tensor([ 0.5352,  0.1562, -0.8008, -0.1719,  0.0781], grad_fn=<ViewBackward0>)\n"]}],"source":["from chop.ir.graph.mase_graph import MaseGraph\n","from chop.passes.graph.utils import get_mase_op, get_mase_type, get_node_actual_target\n","import torch\n","\n","# iterate over original and modified graphs\n","for ori_n, n in zip(ori_mg.fx_graph.nodes, mg.fx_graph.nodes):\n","    # check if the original node and the modified node are the same\n","    # if they arent, then it means that that node has been quantized\n","    if type(get_node_actual_target(n)) != type(get_node_actual_target(ori_n)):\n","        # retrieve the original module from the node\n","        ori_module = get_node_actual_target(ori_n)\n","        # retrieve the quantized module from the node\n","        quant_module = get_node_actual_target(n)\n","\n","        print(type(quant_module))\n","\n","        print(f'Difference found at name: {n.name}, '\n","              f'MASE type: {get_mase_type(n)}, MASE operation: {get_mase_op(n)}\\n'\n","              f'Original module: {type(ori_module)} --> '\n","              f'New module: {type(quant_module)}')\n","        \n","        # Get the precision and types of the weights of the nodes from their metadata\n","        mg_precision = n.meta[\"mase\"].parameters[\"common\"][\"args\"][\"weight\"][\"precision\"]\n","        ori_mg_precision = ori_n.meta[\"mase\"].parameters[\"common\"][\"args\"][\"weight\"][\"precision\"]\n","\n","        mg_type = n.meta[\"mase\"].parameters[\"common\"][\"args\"][\"weight\"][\"type\"]\n","        ori_mg_type = ori_n.meta[\"mase\"].parameters[\"common\"][\"args\"][\"weight\"][\"type\"]\n","\n","        print(f'Precision of original module: {ori_mg_precision}')\n","        print(f'Precision of modified module: {mg_precision}')\n","\n","        # print the weights of the original and quantized modules\n","        print(f'Weight of original module: {ori_module.weight}')\n","        quantized_weights = quant_module.w_quantizer(ori_module.weight)\n","        print(f'Weight of quantized module: {quantized_weights}')\n","\n","        # generate a test input tensor based on the input feature size of the quantized module\n","        test_input = torch.randn(quant_module.in_features)\n","        print(f'Random generated test input: {test_input}')\n","\n","        # apply the original and quantized modules to the test input and print the outputs\n","        print(f'Output for original module: {ori_module(test_input)}')\n","        print(f'Output for quantized module: {quant_module(test_input)}')\n"]},{"cell_type":"markdown","metadata":{"id":"HhTU3755MerQ"},"source":["\n","\n","# Exercises:\n","\n","We have now seen how to:\n","1. Set up a dataset\n","2. Set up a model\n","3. Generate a `MaseGraph` from the model\n","4. Run Analysis and Transform passes on the `MaseGraph`\n","\n","Now consider the following problems:\n","\n","1. Explain the functionality of `report_graph_analysis_pass` and its printed jargons such as `placeholder`, `get_attr` ... You might find the doc of [torch.fx](https://pytorch.org/docs/stable/fx.html) useful.\n","\n","2. What are the functionalities of `profile_statistics_analysis_pass` and `report_node_meta_param_analysis_pass` respectively?\n","\n","## MASE OPs and MASE Types\n","\n","MASE is designed to be a very high-level intermediate representation (IR), this is very different from the classic [LLVM IR](https://llvm.org/docs/LangRef.html) that you might be familiar with.\n","\n","The following MASE Types are available:\n","(Note from Aaron: do we have a page somewhere that have summarized this?)\n","\n","\n","## A deeper dive into the quantisation transform\n","\n","3. Explain why only 1 OP is changed after the `quantize_transform_pass` .\n","\n","4. Write some code to traverse both `mg` and `ori_mg`, check and comment on the nodes in these two graphs. You might find the source code for the implementation of `summarize_quantization_analysis_pass` useful.\n","\n","5. Perform the same quantisation flow to the bigger JSC network that you have trained in lab1. You must be aware that now the `pass_args` for your custom network might be different if you have used more than the `Linear` layer in your network.\n","\n","6. Write code to show and verify that the weights of these layers are indeed quantised. You might need to go through the source code of the implementation of the quantisation pass and also the implementation of the [Quantized Layers](../../machop/chop/passes/transforms/quantize/quantized_modules/linear.py) .\n","\n","## The command line interface\n","\n","The same flow can also be executed on the command line throw the `transform` action.\n","\n","```bash\n","# make sure you have the same printout\n","pwd\n","# it should show\n","# your_dir/mase-tools/machop\n","\n","# enter the following command\n","./ch transform --config configs/examples/jsc_toy_by_type.toml --task cls --cpu=0\n","```\n","7. Load your own pre-trained JSC network, and perform perform the quantisation using the command line interface.\n","\n","## \\[Optional] Write your own pass\n","\n","Many examples of existing passes are in the [source code](../..//machop/chop/passes/__init__.py), the [test files](../../machop/test/passes) for these passes also contain useful information on helping you to understand how these passes are used.\n","\n","Implement a pass to count the number of FLOPs (floating-point operations) and BitOPs (bit-wise operations)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 5."]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["batch_size = 8\n","model_name = \"jsc-medium\"\n","dataset_name = \"jsc\"\n","\n","\n","data_module = MaseDataModule(\n","    name=dataset_name,\n","    batch_size=batch_size,\n","    model_name=model_name,\n","    num_workers=0,\n",")\n","data_module.prepare_data()\n","data_module.setup()"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /Users/jared/Documents/Personal/ICL/courses/ADLS/mase/mase_output/jsc-medium_classification_jsc_2024-01-24/software/training_ckpts/best-v3.ckpt\u001b[0m\n"]}],"source":["# üìùÔ∏è change this CHECKPOINT_PATH to the one you trained in Lab1\n","CHECKPOINT_PATH = \"/Users/jared/Documents/Personal/ICL/courses/ADLS/mase/mase_output/jsc-medium_classification_jsc_2024-01-24/software/training_ckpts/best-v3.ckpt\"\n","model_info = get_model_info(model_name)\n","model = get_model(\n","    model_name,\n","    task=\"cls\",\n","    dataset_info=data_module.dataset_info,\n","    pretrained=False)\n","\n","model = load_model(load_name=CHECKPOINT_PATH, load_type=\"pl\", model=model)"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["# get the input generator\n","input_generator = InputGenerator(\n","    data_module=data_module,\n","    model_info=model_info,\n","    task=\"cls\",\n","    which_dataloader=\"train\",\n",")"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["dummy_in = next(iter(input_generator))"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[36mDEBUG   \u001b[0m \u001b[34mgraph():\n","    %x : [num_users=1] = placeholder[target=x]\n","    %seq_blocks_0 : [num_users=1] = call_module[target=seq_blocks.0](args = (%x,), kwargs = {})\n","    %seq_blocks_1 : [num_users=1] = call_module[target=seq_blocks.1](args = (%seq_blocks_0,), kwargs = {})\n","    %seq_blocks_2 : [num_users=1] = call_module[target=seq_blocks.2](args = (%seq_blocks_1,), kwargs = {})\n","    %seq_blocks_3 : [num_users=1] = call_module[target=seq_blocks.3](args = (%seq_blocks_2,), kwargs = {})\n","    %seq_blocks_4 : [num_users=1] = call_module[target=seq_blocks.4](args = (%seq_blocks_3,), kwargs = {})\n","    %seq_blocks_5 : [num_users=1] = call_module[target=seq_blocks.5](args = (%seq_blocks_4,), kwargs = {})\n","    %seq_blocks_6 : [num_users=1] = call_module[target=seq_blocks.6](args = (%seq_blocks_5,), kwargs = {})\n","    %seq_blocks_7 : [num_users=1] = call_module[target=seq_blocks.7](args = (%seq_blocks_6,), kwargs = {})\n","    %seq_blocks_8 : [num_users=1] = call_module[target=seq_blocks.8](args = (%seq_blocks_7,), kwargs = {})\n","    %seq_blocks_9 : [num_users=1] = call_module[target=seq_blocks.9](args = (%seq_blocks_8,), kwargs = {})\n","    %seq_blocks_10 : [num_users=1] = call_module[target=seq_blocks.10](args = (%seq_blocks_9,), kwargs = {})\n","    %seq_blocks_11 : [num_users=1] = call_module[target=seq_blocks.11](args = (%seq_blocks_10,), kwargs = {})\n","    %seq_blocks_12 : [num_users=1] = call_module[target=seq_blocks.12](args = (%seq_blocks_11,), kwargs = {})\n","    %seq_blocks_13 : [num_users=1] = call_module[target=seq_blocks.13](args = (%seq_blocks_12,), kwargs = {})\n","    return seq_blocks_13\u001b[0m\n"]}],"source":["# generate the mase graph and initialize node metadata\n","mg = MaseGraph(model=model)\n","\n","mg, _ = init_metadata_analysis_pass(mg, None)\n","mg, _ = add_common_metadata_analysis_pass(mg, {\"dummy_in\": dummy_in})\n","mg, _ = add_software_metadata_analysis_pass(mg, None)"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["graph():\n","    %x : [num_users=1] = placeholder[target=x]\n","    %seq_blocks_0 : [num_users=1] = call_module[target=seq_blocks.0](args = (%x,), kwargs = {})\n","    %seq_blocks_1 : [num_users=1] = call_module[target=seq_blocks.1](args = (%seq_blocks_0,), kwargs = {})\n","    %seq_blocks_2 : [num_users=1] = call_module[target=seq_blocks.2](args = (%seq_blocks_1,), kwargs = {})\n","    %seq_blocks_3 : [num_users=1] = call_module[target=seq_blocks.3](args = (%seq_blocks_2,), kwargs = {})\n","    %seq_blocks_4 : [num_users=1] = call_module[target=seq_blocks.4](args = (%seq_blocks_3,), kwargs = {})\n","    %seq_blocks_5 : [num_users=1] = call_module[target=seq_blocks.5](args = (%seq_blocks_4,), kwargs = {})\n","    %seq_blocks_6 : [num_users=1] = call_module[target=seq_blocks.6](args = (%seq_blocks_5,), kwargs = {})\n","    %seq_blocks_7 : [num_users=1] = call_module[target=seq_blocks.7](args = (%seq_blocks_6,), kwargs = {})\n","    %seq_blocks_8 : [num_users=1] = call_module[target=seq_blocks.8](args = (%seq_blocks_7,), kwargs = {})\n","    %seq_blocks_9 : [num_users=1] = call_module[target=seq_blocks.9](args = (%seq_blocks_8,), kwargs = {})\n","    %seq_blocks_10 : [num_users=1] = call_module[target=seq_blocks.10](args = (%seq_blocks_9,), kwargs = {})\n","    %seq_blocks_11 : [num_users=1] = call_module[target=seq_blocks.11](args = (%seq_blocks_10,), kwargs = {})\n","    %seq_blocks_12 : [num_users=1] = call_module[target=seq_blocks.12](args = (%seq_blocks_11,), kwargs = {})\n","    %seq_blocks_13 : [num_users=1] = call_module[target=seq_blocks.13](args = (%seq_blocks_12,), kwargs = {})\n","    return seq_blocks_13\n","Network overview:\n","{'placeholder': 1, 'get_attr': 0, 'call_function': 0, 'call_method': 0, 'call_module': 14, 'output': 1}\n","Layer types:\n","[BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Linear(in_features=16, out_features=32, bias=True), BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Linear(in_features=32, out_features=16, bias=True), BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Linear(in_features=16, out_features=8, bias=True), BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Linear(in_features=8, out_features=5, bias=True), BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]\n"]}],"source":["# report graph is an analysis pass that shows you the detailed information in the graph\n","from chop.passes.graph import report_graph_analysis_pass\n","_ = report_graph_analysis_pass(mg)"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["pass_args = {\n","    \"by\": \"type\",                                                            # collect statistics by node name\n","    \"target_weight_nodes\": [\"linear\"],                                       # collect weight statistics for linear layers\n","    \"target_activation_nodes\": [\"relu\"],                                     # collect activation statistics for relu layers\n","    \"weight_statistics\": {\n","        \"variance_precise\": {\"device\": \"cpu\", \"dims\": \"all\"},                # collect precise variance of the weight\n","    },\n","    \"activation_statistics\": {\n","        \"range_quantile\": {\"device\": \"cpu\", \"dims\": \"all\", \"quantile\": 0.97} # collect 97% quantile of the activation range\n","    },\n","    \"input_generator\": input_generator,                                      # the input generator for feeding data to the model\n","    \"num_samples\": 32,                                                       # feed 32 samples to the model\n","}"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Profiling weight statistics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 17957.95it/s]\n","Profiling act statistics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 249.64it/s]\n","\u001b[32mINFO    \u001b[0m \u001b[34mInspecting graph [add_common_meta_param_analysis_pass]\u001b[0m\n","\u001b[32mINFO    \u001b[0m \u001b[34m\n","+---------------+--------------+---------------------+--------------+-----------------------------------------------------------------------------------------+\n","| Node name     | Fx Node op   | Mase type           | Mase op      | Software Param                                                                          |\n","+===============+==============+=====================+==============+=========================================================================================+\n","| x             | placeholder  | placeholder         | placeholder  | {'results': {'data_out_0': {'stat': {}}}}                                               |\n","+---------------+--------------+---------------------+--------------+-----------------------------------------------------------------------------------------+\n","| seq_blocks_0  | call_module  | module              | batch_norm1d | {'args': {'bias': {'stat': {}},                                                         |\n","|               |              |                     |              |           'data_in_0': {'stat': {}},                                                    |\n","|               |              |                     |              |           'running_mean': {'stat': {}},                                                 |\n","|               |              |                     |              |           'running_var': {'stat': {}},                                                  |\n","|               |              |                     |              |           'weight': {'stat': {}}},                                                      |\n","|               |              |                     |              |  'results': {'data_out_0': {'stat': {}}}}                                               |\n","+---------------+--------------+---------------------+--------------+-----------------------------------------------------------------------------------------+\n","| seq_blocks_1  | call_module  | module_related_func | relu         | {'args': {'data_in_0': {'stat': {'range_quantile': {'count': 512,                       |\n","|               |              |                     |              |                                                     'max': 38.24542236328125,           |\n","|               |              |                     |              |                                                     'min': -29.385786056518555,         |\n","|               |              |                     |              |                                                     'range': 67.63121032714844}}}},     |\n","|               |              |                     |              |  'results': {'data_out_0': {'stat': {}}}}                                               |\n","+---------------+--------------+---------------------+--------------+-----------------------------------------------------------------------------------------+\n","| seq_blocks_2  | call_module  | module_related_func | linear       | {'args': {'bias': {'stat': {'variance_precise': {'count': 32,                           |\n","|               |              |                     |              |                                                  'mean': -0.01602105051279068,          |\n","|               |              |                     |              |                                                  'variance': 0.017917029559612274}}},   |\n","|               |              |                     |              |           'data_in_0': {'stat': {}},                                                    |\n","|               |              |                     |              |           'weight': {'stat': {'variance_precise': {'count': 512,                        |\n","|               |              |                     |              |                                                    'mean': -0.004948016256093979,       |\n","|               |              |                     |              |                                                    'variance': 0.06502748280763626}}}}, |\n","|               |              |                     |              |  'results': {'data_out_0': {'stat': {}}}}                                               |\n","+---------------+--------------+---------------------+--------------+-----------------------------------------------------------------------------------------+\n","| seq_blocks_3  | call_module  | module              | batch_norm1d | {'args': {'bias': {'stat': {}},                                                         |\n","|               |              |                     |              |           'data_in_0': {'stat': {}},                                                    |\n","|               |              |                     |              |           'running_mean': {'stat': {}},                                                 |\n","|               |              |                     |              |           'running_var': {'stat': {}},                                                  |\n","|               |              |                     |              |           'weight': {'stat': {}}},                                                      |\n","|               |              |                     |              |  'results': {'data_out_0': {'stat': {}}}}                                               |\n","+---------------+--------------+---------------------+--------------+-----------------------------------------------------------------------------------------+\n","| seq_blocks_4  | call_module  | module_related_func | relu         | {'args': {'data_in_0': {'stat': {'range_quantile': {'count': 1024,                      |\n","|               |              |                     |              |                                                     'max': 1.6592425107955933,          |\n","|               |              |                     |              |                                                     'min': -1.9071587324142456,         |\n","|               |              |                     |              |                                                     'range': 3.566401243209839}}}},     |\n","|               |              |                     |              |  'results': {'data_out_0': {'stat': {}}}}                                               |\n","+---------------+--------------+---------------------+--------------+-----------------------------------------------------------------------------------------+\n","| seq_blocks_5  | call_module  | module_related_func | linear       | {'args': {'bias': {'stat': {'variance_precise': {'count': 16,                           |\n","|               |              |                     |              |                                                  'mean': 0.02759937383234501,           |\n","|               |              |                     |              |                                                  'variance': 0.011591500602662563}}},   |\n","|               |              |                     |              |           'data_in_0': {'stat': {}},                                                    |\n","|               |              |                     |              |           'weight': {'stat': {'variance_precise': {'count': 512,                        |\n","|               |              |                     |              |                                                    'mean': -0.012910411693155766,       |\n","|               |              |                     |              |                                                    'variance': 0.04561278596520424}}}}, |\n","|               |              |                     |              |  'results': {'data_out_0': {'stat': {}}}}                                               |\n","+---------------+--------------+---------------------+--------------+-----------------------------------------------------------------------------------------+\n","| seq_blocks_6  | call_module  | module              | batch_norm1d | {'args': {'bias': {'stat': {}},                                                         |\n","|               |              |                     |              |           'data_in_0': {'stat': {}},                                                    |\n","|               |              |                     |              |           'running_mean': {'stat': {}},                                                 |\n","|               |              |                     |              |           'running_var': {'stat': {}},                                                  |\n","|               |              |                     |              |           'weight': {'stat': {}}},                                                      |\n","|               |              |                     |              |  'results': {'data_out_0': {'stat': {}}}}                                               |\n","+---------------+--------------+---------------------+--------------+-----------------------------------------------------------------------------------------+\n","| seq_blocks_7  | call_module  | module_related_func | relu         | {'args': {'data_in_0': {'stat': {'range_quantile': {'count': 512,                       |\n","|               |              |                     |              |                                                     'max': 1.75718355178833,            |\n","|               |              |                     |              |                                                     'min': -1.6829675436019897,         |\n","|               |              |                     |              |                                                     'range': 3.4401512145996094}}}},    |\n","|               |              |                     |              |  'results': {'data_out_0': {'stat': {}}}}                                               |\n","+---------------+--------------+---------------------+--------------+-----------------------------------------------------------------------------------------+\n","| seq_blocks_8  | call_module  | module_related_func | linear       | {'args': {'bias': {'stat': {'variance_precise': {'count': 8,                            |\n","|               |              |                     |              |                                                  'mean': 0.034346286207437515,          |\n","|               |              |                     |              |                                                  'variance': 0.024107107892632484}}},   |\n","|               |              |                     |              |           'data_in_0': {'stat': {}},                                                    |\n","|               |              |                     |              |           'weight': {'stat': {'variance_precise': {'count': 128,                        |\n","|               |              |                     |              |                                                    'mean': -0.0456107035279274,         |\n","|               |              |                     |              |                                                    'variance': 0.05062663182616234}}}}, |\n","|               |              |                     |              |  'results': {'data_out_0': {'stat': {}}}}                                               |\n","+---------------+--------------+---------------------+--------------+-----------------------------------------------------------------------------------------+\n","| seq_blocks_9  | call_module  | module              | batch_norm1d | {'args': {'bias': {'stat': {}},                                                         |\n","|               |              |                     |              |           'data_in_0': {'stat': {}},                                                    |\n","|               |              |                     |              |           'running_mean': {'stat': {}},                                                 |\n","|               |              |                     |              |           'running_var': {'stat': {}},                                                  |\n","|               |              |                     |              |           'weight': {'stat': {}}},                                                      |\n","|               |              |                     |              |  'results': {'data_out_0': {'stat': {}}}}                                               |\n","+---------------+--------------+---------------------+--------------+-----------------------------------------------------------------------------------------+\n","| seq_blocks_10 | call_module  | module_related_func | relu         | {'args': {'data_in_0': {'stat': {'range_quantile': {'count': 256,                       |\n","|               |              |                     |              |                                                     'max': 1.7349841594696045,          |\n","|               |              |                     |              |                                                     'min': -1.6942534446716309,         |\n","|               |              |                     |              |                                                     'range': 3.4292376041412354}}}},    |\n","|               |              |                     |              |  'results': {'data_out_0': {'stat': {}}}}                                               |\n","+---------------+--------------+---------------------+--------------+-----------------------------------------------------------------------------------------+\n","| seq_blocks_11 | call_module  | module_related_func | linear       | {'args': {'bias': {'stat': {'variance_precise': {'count': 5,                            |\n","|               |              |                     |              |                                                  'mean': 0.16404694318771362,           |\n","|               |              |                     |              |                                                  'variance': 0.006622841116040945}}},   |\n","|               |              |                     |              |           'data_in_0': {'stat': {}},                                                    |\n","|               |              |                     |              |           'weight': {'stat': {'variance_precise': {'count': 40,                         |\n","|               |              |                     |              |                                                    'mean': -0.030284583568572998,       |\n","|               |              |                     |              |                                                    'variance': 0.07175514847040176}}}}, |\n","|               |              |                     |              |  'results': {'data_out_0': {'stat': {}}}}                                               |\n","+---------------+--------------+---------------------+--------------+-----------------------------------------------------------------------------------------+\n","| seq_blocks_12 | call_module  | module              | batch_norm1d | {'args': {'bias': {'stat': {}},                                                         |\n","|               |              |                     |              |           'data_in_0': {'stat': {}},                                                    |\n","|               |              |                     |              |           'running_mean': {'stat': {}},                                                 |\n","|               |              |                     |              |           'running_var': {'stat': {}},                                                  |\n","|               |              |                     |              |           'weight': {'stat': {}}},                                                      |\n","|               |              |                     |              |  'results': {'data_out_0': {'stat': {}}}}                                               |\n","+---------------+--------------+---------------------+--------------+-----------------------------------------------------------------------------------------+\n","| seq_blocks_13 | call_module  | module_related_func | relu         | {'args': {'data_in_0': {'stat': {'range_quantile': {'count': 160,                       |\n","|               |              |                     |              |                                                     'max': 5.679187297821045,           |\n","|               |              |                     |              |                                                     'min': -2.0914652347564697,         |\n","|               |              |                     |              |                                                     'range': 7.770652770996094}}}},     |\n","|               |              |                     |              |  'results': {'data_out_0': {'stat': {}}}}                                               |\n","+---------------+--------------+---------------------+--------------+-----------------------------------------------------------------------------------------+\n","| output        | output       | output              | output       | {'args': {'data_in_0': {'stat': {}}}}                                                   |\n","+---------------+--------------+---------------------+--------------+-----------------------------------------------------------------------------------------+\u001b[0m\n"]}],"source":["mg, _ = profile_statistics_analysis_pass(mg, pass_args)\n","mg, _ = report_node_meta_param_analysis_pass(mg, {\"which\": (\"software\",)})"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["pass_args = {\n","    \"by\": \"type\",\n","    \"default\": {\"config\": {\"name\": None}},\n","    \"linear\": {\n","        \"config\": {\n","            \"name\": \"integer\",\n","            # data\n","            \"data_in_width\": 8,\n","            \"data_in_frac_width\": 4,\n","            # weight\n","            \"weight_width\": 8,\n","            \"weight_frac_width\": 4,\n","            # bias\n","            \"bias_width\": 8,\n","            \"bias_frac_width\": 4,\n","        }\n","    },\n","}"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[36mDEBUG   \u001b[0m \u001b[34mgraph():\n","    %x : [num_users=1] = placeholder[target=x]\n","    %seq_blocks_0 : [num_users=1] = call_module[target=seq_blocks.0](args = (%x,), kwargs = {})\n","    %seq_blocks_1 : [num_users=1] = call_module[target=seq_blocks.1](args = (%seq_blocks_0,), kwargs = {})\n","    %seq_blocks_2 : [num_users=1] = call_module[target=seq_blocks.2](args = (%seq_blocks_1,), kwargs = {})\n","    %seq_blocks_3 : [num_users=1] = call_module[target=seq_blocks.3](args = (%seq_blocks_2,), kwargs = {})\n","    %seq_blocks_4 : [num_users=1] = call_module[target=seq_blocks.4](args = (%seq_blocks_3,), kwargs = {})\n","    %seq_blocks_5 : [num_users=1] = call_module[target=seq_blocks.5](args = (%seq_blocks_4,), kwargs = {})\n","    %seq_blocks_6 : [num_users=1] = call_module[target=seq_blocks.6](args = (%seq_blocks_5,), kwargs = {})\n","    %seq_blocks_7 : [num_users=1] = call_module[target=seq_blocks.7](args = (%seq_blocks_6,), kwargs = {})\n","    %seq_blocks_8 : [num_users=1] = call_module[target=seq_blocks.8](args = (%seq_blocks_7,), kwargs = {})\n","    %seq_blocks_9 : [num_users=1] = call_module[target=seq_blocks.9](args = (%seq_blocks_8,), kwargs = {})\n","    %seq_blocks_10 : [num_users=1] = call_module[target=seq_blocks.10](args = (%seq_blocks_9,), kwargs = {})\n","    %seq_blocks_11 : [num_users=1] = call_module[target=seq_blocks.11](args = (%seq_blocks_10,), kwargs = {})\n","    %seq_blocks_12 : [num_users=1] = call_module[target=seq_blocks.12](args = (%seq_blocks_11,), kwargs = {})\n","    %seq_blocks_13 : [num_users=1] = call_module[target=seq_blocks.13](args = (%seq_blocks_12,), kwargs = {})\n","    return seq_blocks_13\u001b[0m\n","\u001b[36mDEBUG   \u001b[0m \u001b[34mCompare nodes:\u001b[0m\n","\u001b[36mDEBUG   \u001b[0m \u001b[34m\n","| Ori name      | New name      | MASE_TYPE           | Mase_OP      | Original type   | Quantized type   | Changed   |\n","|---------------+---------------+---------------------+--------------+-----------------+------------------+-----------|\n","| x             | x             | placeholder         | placeholder  | x               | x                | False     |\n","| seq_blocks_0  | seq_blocks_0  | module              | batch_norm1d | BatchNorm1d     | BatchNorm1d      | False     |\n","| seq_blocks_1  | seq_blocks_1  | module_related_func | relu         | ReLU            | ReLU             | False     |\n","| seq_blocks_2  | seq_blocks_2  | module_related_func | linear       | Linear          | LinearInteger    | True      |\n","| seq_blocks_3  | seq_blocks_3  | module              | batch_norm1d | BatchNorm1d     | BatchNorm1d      | False     |\n","| seq_blocks_4  | seq_blocks_4  | module_related_func | relu         | ReLU            | ReLU             | False     |\n","| seq_blocks_5  | seq_blocks_5  | module_related_func | linear       | Linear          | LinearInteger    | True      |\n","| seq_blocks_6  | seq_blocks_6  | module              | batch_norm1d | BatchNorm1d     | BatchNorm1d      | False     |\n","| seq_blocks_7  | seq_blocks_7  | module_related_func | relu         | ReLU            | ReLU             | False     |\n","| seq_blocks_8  | seq_blocks_8  | module_related_func | linear       | Linear          | LinearInteger    | True      |\n","| seq_blocks_9  | seq_blocks_9  | module              | batch_norm1d | BatchNorm1d     | BatchNorm1d      | False     |\n","| seq_blocks_10 | seq_blocks_10 | module_related_func | relu         | ReLU            | ReLU             | False     |\n","| seq_blocks_11 | seq_blocks_11 | module_related_func | linear       | Linear          | LinearInteger    | True      |\n","| seq_blocks_12 | seq_blocks_12 | module              | batch_norm1d | BatchNorm1d     | BatchNorm1d      | False     |\n","| seq_blocks_13 | seq_blocks_13 | module_related_func | relu         | ReLU            | ReLU             | False     |\n","| output        | output        | output              | output       | output          | output           | False     |\u001b[0m\n","\u001b[32mINFO    \u001b[0m \u001b[34mQuantized graph histogram:\u001b[0m\n","\u001b[32mINFO    \u001b[0m \u001b[34m\n","| Original type   | OP           |   Total |   Changed |   Unchanged |\n","|-----------------+--------------+---------+-----------+-------------|\n","| BatchNorm1d     | batch_norm1d |       5 |         0 |           5 |\n","| Linear          | linear       |       4 |         4 |           0 |\n","| ReLU            | relu         |       5 |         0 |           5 |\n","| output          | output       |       1 |         0 |           1 |\n","| x               | placeholder  |       1 |         0 |           1 |\u001b[0m\n"]}],"source":["from chop.passes.graph.transforms import (\n","    quantize_transform_pass,\n","    summarize_quantization_analysis_pass,\n",")\n","from chop.ir.graph.mase_graph import MaseGraph\n","\n","ori_mg = MaseGraph(model=model)\n","ori_mg, _ = init_metadata_analysis_pass(ori_mg, None)\n","ori_mg, _ = add_common_metadata_analysis_pass(ori_mg, {\"dummy_in\": dummy_in})\n","\n","mg, _ = quantize_transform_pass(mg, pass_args)\n","summarize_quantization_analysis_pass(ori_mg, mg, save_dir=\"quantize_summary\")"]},{"cell_type":"markdown","metadata":{},"source":["# 6."]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Difference found at name: seq_blocks_2, MASE type: module_related_func, MASE operation: linear\n","Original module: <class 'torch.nn.modules.linear.Linear'> --> New module: <class 'chop.passes.graph.transforms.quantize.quantized_modules.linear.LinearInteger'>\n","Precision of original module: [32]\n","Precision of modified module: [8, 4]\n","Type of original module: float\n","Tyoe of modified module: integer\n","Weight of original module: Parameter containing:\n","tensor([[-2.0059e-01,  7.0937e-02, -9.0220e-01, -3.3770e-01, -2.1713e-01,\n","         -4.4070e-01, -1.0396e-02, -2.6671e-02,  1.2821e-01,  5.0434e-01,\n","          6.0154e-02, -9.6376e-02, -1.2634e-01, -5.2315e-02,  1.1104e-01,\n","         -1.4309e-02],\n","        [-5.8988e-02,  7.6669e-02,  1.1914e-01,  1.6312e-01, -3.9841e-01,\n","          1.9455e-01,  1.9242e-02,  2.9598e-01,  1.4485e-01, -1.7565e-01,\n","          1.7875e-01, -2.9895e-01,  6.2455e-02, -2.7527e-01, -9.8624e-02,\n","          6.7748e-02],\n","        [-6.6721e-02,  5.2548e-02, -4.9106e-01, -3.0251e-01,  3.0298e-01,\n","          1.7063e-01,  8.4487e-02,  6.0341e-02,  7.6235e-02, -1.6422e-01,\n","         -4.4930e-02, -2.4269e-01, -1.1802e-01, -2.9312e-01,  2.6968e-01,\n","         -1.1605e-02],\n","        [ 1.7765e-01,  1.4337e-04,  5.8556e-01,  2.9210e-01, -4.0488e-01,\n","          1.5749e-01, -5.0958e-02, -3.6898e-02, -3.4969e-02, -2.1817e-01,\n","         -2.5161e-02, -5.8310e-02,  2.1869e-01, -1.1095e-01, -5.9321e-01,\n","          4.2452e-05],\n","        [ 2.0397e-01, -8.7888e-03,  2.7631e-01, -1.0005e-01,  4.2933e-02,\n","          5.7423e-01, -7.8329e-03,  9.6181e-02,  1.3335e-01, -3.6843e-02,\n","          8.5629e-03,  2.1146e-01, -2.8945e-02, -1.8978e-01, -8.5525e-02,\n","          9.1451e-02],\n","        [ 5.9642e-02, -1.2030e-02, -2.2063e-01, -1.0121e+00,  3.8642e-02,\n","         -4.3222e-01, -9.6167e-03, -1.3463e-02, -2.1588e-02, -3.7499e-01,\n","         -5.3905e-02,  1.3014e-01,  1.2544e-02,  7.9407e-02, -1.6949e-01,\n","          3.7900e-02],\n","        [ 1.2626e-01,  1.2629e-01, -2.8521e-01,  1.7010e-01, -4.1231e-02,\n","          3.2585e-01, -7.2340e-02, -5.9621e-02, -1.3556e-02,  1.5605e-01,\n","         -3.6087e-03, -1.4235e-01,  1.7300e-02, -5.1551e-02, -9.2693e-01,\n","          8.1758e-02],\n","        [-3.8899e-02,  5.4048e-02, -2.4617e-01, -3.1919e-01,  3.9084e-01,\n","          2.8990e-01, -6.8768e-02,  1.2402e-02, -1.3238e-01, -2.0666e-02,\n","         -1.3440e-01,  1.5782e-01, -2.4401e-01, -1.8689e-01, -5.5418e-01,\n","          7.4639e-02],\n","        [ 4.6317e-01,  3.6642e-01, -2.4689e-02,  2.8054e-01,  8.3301e-03,\n","          4.9456e-02, -4.5266e-02,  5.3558e-03,  3.7972e-02, -1.4745e-01,\n","          1.9983e-01,  1.6850e-01, -1.5898e-01, -8.2602e-02,  1.7561e-01,\n","         -5.1536e-01],\n","        [-1.1896e-01,  5.4313e-02,  5.7660e-01, -2.2969e-02,  2.3570e-01,\n","         -5.4054e-01, -6.0586e-02, -2.1382e-01, -3.7689e-03,  1.4558e-01,\n","          1.4677e-01, -1.8231e-01,  6.6321e-03, -2.1710e-01,  4.2843e-01,\n","          2.3521e-01],\n","        [-4.7850e-02,  3.0811e-03,  2.8922e-02, -1.3289e-01,  7.5705e-01,\n","          7.0069e-01, -7.7228e-04, -3.9782e-02,  8.3352e-02, -1.2090e-01,\n","         -3.5210e-02, -7.3726e-02, -3.3544e-02,  5.8560e-02, -9.1811e-02,\n","          4.0592e-02],\n","        [-3.1566e-03,  7.7775e-02,  4.0415e-01,  2.9758e-01,  4.9813e-01,\n","          3.0583e-01,  5.2763e-02,  2.2709e-02, -5.2243e-02, -5.1048e-02,\n","         -5.0498e-03, -5.5277e-02, -5.7427e-02, -7.3168e-02, -1.0819e-01,\n","         -5.1632e-02],\n","        [-3.1210e-01,  6.6336e-02, -1.3284e-01, -4.6676e-01, -2.0605e-02,\n","          3.5000e-01,  3.6194e-02,  2.3955e-02, -6.7453e-03,  5.5884e-03,\n","         -8.1097e-03, -1.4790e-01, -2.0852e-01, -2.6397e-01, -4.5408e-02,\n","          4.6907e-01],\n","        [ 6.7421e-01, -1.7944e-01,  2.9643e-02,  3.7234e-01, -3.2145e-01,\n","         -1.6300e-01,  1.0448e-01,  9.5591e-03, -9.2681e-03,  5.9645e-02,\n","          7.1186e-02, -1.8952e-01, -1.1676e-02,  3.6753e-01,  1.5833e-01,\n","          2.7272e-01],\n","        [ 2.4580e-01,  1.0944e+00,  2.0972e-01, -1.6592e-01, -1.5506e-03,\n","          1.1730e-01,  3.3471e-02,  3.2407e-02, -2.8816e-03, -1.7589e-01,\n","         -2.0137e-02, -1.9553e-01,  1.5791e-01, -6.2062e-02, -9.0140e-02,\n","         -6.1251e-02],\n","        [ 1.0233e-01,  8.9854e-02,  1.5912e-01, -8.6085e-01, -2.6688e-01,\n","         -5.8660e-01, -4.7901e-02, -4.2384e-03, -1.9764e-02, -1.2603e-01,\n","          2.1983e-02,  5.9897e-02,  3.5478e-02, -4.2340e-02,  4.7219e-02,\n","         -3.2137e-02],\n","        [ 2.0020e-01,  1.6558e-01, -2.4486e-02, -1.5050e-01,  2.8481e-01,\n","          2.7242e-02,  3.7524e-02, -2.0719e-02,  1.1922e-01,  1.2118e-01,\n","         -5.9027e-02,  5.5232e-02,  7.7847e-04,  4.1753e-02,  1.9697e-01,\n","         -6.8511e-01],\n","        [-2.5680e-01, -4.0553e-02, -1.0061e+00, -4.0851e-01,  6.2976e-01,\n","         -1.0653e-01,  5.0339e-02, -9.1563e-02, -8.7936e-03, -1.4234e-01,\n","         -1.1332e-01,  2.3134e-01,  1.0171e-01,  1.8080e-01, -1.0018e-04,\n","         -2.4517e-01],\n","        [-1.9594e-03,  2.3613e-01, -7.5785e-03, -1.8781e-01,  4.8717e-01,\n","          4.2218e-01, -1.9048e-02, -5.8856e-02, -6.9094e-03, -4.4130e-01,\n","          7.4568e-03, -3.2101e-02, -6.9265e-02,  2.3161e-01, -3.8984e-01,\n","          1.4813e-01],\n","        [ 1.1139e-01,  2.2572e-01,  4.5279e-02,  4.6271e-01, -3.1994e-01,\n","         -2.5982e-01, -4.9823e-03,  6.7048e-02,  7.1614e-02, -6.8814e-02,\n","         -1.3279e-02,  1.6025e-01, -6.9438e-02,  2.4857e-01,  7.2282e-02,\n","         -3.4878e-01],\n","        [-1.0125e-01, -2.5074e-03,  4.2796e-03,  1.8319e-01, -1.9333e-01,\n","          4.7450e-01,  8.5678e-02, -7.8711e-02, -1.5341e-01, -7.7191e-01,\n","         -7.3733e-02, -1.2606e-01,  1.2681e-01, -7.1683e-02, -4.6067e-01,\n","          6.0593e-02],\n","        [ 2.0854e-01,  5.5756e-01,  1.0758e-01,  1.5331e-01, -3.8276e-01,\n","          6.9252e-01, -6.2024e-03,  5.8844e-02, -1.2077e-01,  1.0147e-01,\n","          1.2220e-01,  9.8703e-02, -6.0578e-02, -1.8373e-01, -9.4474e-02,\n","          4.4981e-01],\n","        [ 4.7199e-01,  2.3263e-01,  1.2653e-02,  1.2315e-02,  1.9865e-01,\n","         -4.8553e-02, -9.0715e-02, -9.2154e-02,  4.6556e-02,  3.0430e-01,\n","          1.4070e-02,  2.0461e-01, -3.7386e-02,  5.9263e-01, -4.9833e-02,\n","         -1.1189e-01],\n","        [-4.6865e-02,  4.3867e-02,  2.6694e-01,  9.5424e-02, -6.5953e-01,\n","         -4.7973e-01, -1.9556e-02, -3.5709e-03,  5.0422e-01,  2.3339e-01,\n","          8.3234e-02,  1.2063e-01,  1.6169e-01, -2.4385e-01, -1.3265e-01,\n","         -2.2362e-01],\n","        [ 2.0066e-01,  1.3273e-02, -9.6549e-02,  5.6670e-01, -1.4434e-01,\n","          5.4448e-02,  3.9324e-02, -3.6271e-02, -6.2628e-02, -3.0779e-01,\n","         -1.0597e-01,  1.2995e-01,  2.4835e-02, -1.9039e-01,  1.7803e-01,\n","          5.8692e-01],\n","        [-3.5334e-03,  1.5600e-01,  2.6022e-01, -3.8902e-01, -4.2761e-01,\n","         -1.0709e+00, -8.2554e-03,  4.1584e-02, -2.0003e-01, -1.0091e-03,\n","         -2.2462e-02, -4.9200e-02, -7.2636e-02, -1.0734e-01, -8.8298e-02,\n","         -5.1474e-02],\n","        [ 1.7638e-02, -1.4447e-01,  3.5353e-02,  5.7085e-01, -2.9861e-02,\n","         -3.9249e-01,  7.2390e-03, -1.2640e-04,  1.2426e-01, -8.6779e-02,\n","         -1.7023e-02, -2.0655e-01, -1.0530e-01,  2.0993e-01, -2.1015e-01,\n","         -2.9481e-01],\n","        [-2.0774e-01,  1.2644e-01, -2.9095e-01, -6.0485e-01,  1.5578e-01,\n","         -4.5455e-01,  6.2546e-03, -7.8750e-02,  8.1266e-02,  1.7562e-01,\n","         -7.3517e-02,  1.5760e-01, -2.3775e-01,  2.0150e-01,  1.6808e-01,\n","          7.4029e-04],\n","        [-7.0307e-02, -8.1635e-02,  5.4098e-02,  2.7457e-01, -2.8762e-01,\n","         -5.6760e-02, -6.7686e-02,  2.0108e-02,  5.4072e-02,  3.3201e-01,\n","         -5.9626e-03,  8.0956e-02, -1.3376e-01,  2.8403e-02, -7.0960e-01,\n","         -3.0078e-02],\n","        [-1.6936e-01, -1.4551e-01, -5.0098e-02, -5.5542e-01, -2.9871e-01,\n","         -2.8162e-01,  1.0106e-01,  1.6375e-01,  8.6139e-02,  4.1048e-01,\n","         -9.9064e-02, -3.0374e-01, -1.3352e-01,  4.4159e-01, -1.7307e-02,\n","         -8.4360e-02],\n","        [ 7.3594e-02, -1.1736e-01,  2.2532e-01,  2.5168e-01,  7.6203e-02,\n","          2.3858e-01,  6.5483e-02,  7.9023e-03, -3.1687e-02, -2.4319e-01,\n","         -1.3380e-02,  6.9842e-02,  1.9251e-01, -1.9482e-01,  5.4571e-01,\n","         -6.7600e-02],\n","        [-9.8188e-02, -6.5727e-02,  7.4846e-01,  1.9858e-01, -5.2381e-01,\n","         -4.9542e-01, -1.2697e-01, -6.3248e-02,  5.3741e-04,  1.0860e-01,\n","          2.0731e-02, -2.9215e-02, -2.7521e-01,  1.9574e-01,  1.3199e-02,\n","          6.6513e-02]], requires_grad=True)\n","Weight of quantized module: tensor([[-0.1875,  0.0625, -0.8750, -0.3125, -0.1875, -0.4375, -0.0000, -0.0000,\n","          0.1250,  0.5000,  0.0625, -0.1250, -0.1250, -0.0625,  0.1250, -0.0000],\n","        [-0.0625,  0.0625,  0.1250,  0.1875, -0.3750,  0.1875,  0.0000,  0.3125,\n","          0.1250, -0.1875,  0.1875, -0.3125,  0.0625, -0.2500, -0.1250,  0.0625],\n","        [-0.0625,  0.0625, -0.5000, -0.3125,  0.3125,  0.1875,  0.0625,  0.0625,\n","          0.0625, -0.1875, -0.0625, -0.2500, -0.1250, -0.3125,  0.2500, -0.0000],\n","        [ 0.1875,  0.0000,  0.5625,  0.3125, -0.3750,  0.1875, -0.0625, -0.0625,\n","         -0.0625, -0.1875, -0.0000, -0.0625,  0.1875, -0.1250, -0.5625,  0.0000],\n","        [ 0.1875, -0.0000,  0.2500, -0.1250,  0.0625,  0.5625, -0.0000,  0.1250,\n","          0.1250, -0.0625,  0.0000,  0.1875, -0.0000, -0.1875, -0.0625,  0.0625],\n","        [ 0.0625, -0.0000, -0.2500, -1.0000,  0.0625, -0.4375, -0.0000, -0.0000,\n","         -0.0000, -0.3750, -0.0625,  0.1250,  0.0000,  0.0625, -0.1875,  0.0625],\n","        [ 0.1250,  0.1250, -0.3125,  0.1875, -0.0625,  0.3125, -0.0625, -0.0625,\n","         -0.0000,  0.1250, -0.0000, -0.1250,  0.0000, -0.0625, -0.9375,  0.0625],\n","        [-0.0625,  0.0625, -0.2500, -0.3125,  0.3750,  0.3125, -0.0625,  0.0000,\n","         -0.1250, -0.0000, -0.1250,  0.1875, -0.2500, -0.1875, -0.5625,  0.0625],\n","        [ 0.4375,  0.3750, -0.0000,  0.2500,  0.0000,  0.0625, -0.0625,  0.0000,\n","          0.0625, -0.1250,  0.1875,  0.1875, -0.1875, -0.0625,  0.1875, -0.5000],\n","        [-0.1250,  0.0625,  0.5625, -0.0000,  0.2500, -0.5625, -0.0625, -0.1875,\n","         -0.0000,  0.1250,  0.1250, -0.1875,  0.0000, -0.1875,  0.4375,  0.2500],\n","        [-0.0625,  0.0000,  0.0000, -0.1250,  0.7500,  0.6875, -0.0000, -0.0625,\n","          0.0625, -0.1250, -0.0625, -0.0625, -0.0625,  0.0625, -0.0625,  0.0625],\n","        [-0.0000,  0.0625,  0.3750,  0.3125,  0.5000,  0.3125,  0.0625,  0.0000,\n","         -0.0625, -0.0625, -0.0000, -0.0625, -0.0625, -0.0625, -0.1250, -0.0625],\n","        [-0.3125,  0.0625, -0.1250, -0.4375, -0.0000,  0.3750,  0.0625,  0.0000,\n","         -0.0000,  0.0000, -0.0000, -0.1250, -0.1875, -0.2500, -0.0625,  0.5000],\n","        [ 0.6875, -0.1875,  0.0000,  0.3750, -0.3125, -0.1875,  0.1250,  0.0000,\n","         -0.0000,  0.0625,  0.0625, -0.1875, -0.0000,  0.3750,  0.1875,  0.2500],\n","        [ 0.2500,  1.1250,  0.1875, -0.1875, -0.0000,  0.1250,  0.0625,  0.0625,\n","         -0.0000, -0.1875, -0.0000, -0.1875,  0.1875, -0.0625, -0.0625, -0.0625],\n","        [ 0.1250,  0.0625,  0.1875, -0.8750, -0.2500, -0.5625, -0.0625, -0.0000,\n","         -0.0000, -0.1250,  0.0000,  0.0625,  0.0625, -0.0625,  0.0625, -0.0625],\n","        [ 0.1875,  0.1875, -0.0000, -0.1250,  0.3125,  0.0000,  0.0625, -0.0000,\n","          0.1250,  0.1250, -0.0625,  0.0625,  0.0000,  0.0625,  0.1875, -0.6875],\n","        [-0.2500, -0.0625, -1.0000, -0.4375,  0.6250, -0.1250,  0.0625, -0.0625,\n","         -0.0000, -0.1250, -0.1250,  0.2500,  0.1250,  0.1875, -0.0000, -0.2500],\n","        [-0.0000,  0.2500, -0.0000, -0.1875,  0.5000,  0.4375, -0.0000, -0.0625,\n","         -0.0000, -0.4375,  0.0000, -0.0625, -0.0625,  0.2500, -0.3750,  0.1250],\n","        [ 0.1250,  0.2500,  0.0625,  0.4375, -0.3125, -0.2500, -0.0000,  0.0625,\n","          0.0625, -0.0625, -0.0000,  0.1875, -0.0625,  0.2500,  0.0625, -0.3750],\n","        [-0.1250, -0.0000,  0.0000,  0.1875, -0.1875,  0.5000,  0.0625, -0.0625,\n","         -0.1250, -0.7500, -0.0625, -0.1250,  0.1250, -0.0625, -0.4375,  0.0625],\n","        [ 0.1875,  0.5625,  0.1250,  0.1250, -0.3750,  0.6875, -0.0000,  0.0625,\n","         -0.1250,  0.1250,  0.1250,  0.1250, -0.0625, -0.1875, -0.1250,  0.4375],\n","        [ 0.5000,  0.2500,  0.0000,  0.0000,  0.1875, -0.0625, -0.0625, -0.0625,\n","          0.0625,  0.3125,  0.0000,  0.1875, -0.0625,  0.5625, -0.0625, -0.1250],\n","        [-0.0625,  0.0625,  0.2500,  0.1250, -0.6875, -0.5000, -0.0000, -0.0000,\n","          0.5000,  0.2500,  0.0625,  0.1250,  0.1875, -0.2500, -0.1250, -0.2500],\n","        [ 0.1875,  0.0000, -0.1250,  0.5625, -0.1250,  0.0625,  0.0625, -0.0625,\n","         -0.0625, -0.3125, -0.1250,  0.1250,  0.0000, -0.1875,  0.1875,  0.5625],\n","        [-0.0000,  0.1250,  0.2500, -0.3750, -0.4375, -1.0625, -0.0000,  0.0625,\n","         -0.1875, -0.0000, -0.0000, -0.0625, -0.0625, -0.1250, -0.0625, -0.0625],\n","        [ 0.0000, -0.1250,  0.0625,  0.5625, -0.0000, -0.3750,  0.0000, -0.0000,\n","          0.1250, -0.0625, -0.0000, -0.1875, -0.1250,  0.1875, -0.1875, -0.3125],\n","        [-0.1875,  0.1250, -0.3125, -0.6250,  0.1250, -0.4375,  0.0000, -0.0625,\n","          0.0625,  0.1875, -0.0625,  0.1875, -0.2500,  0.1875,  0.1875,  0.0000],\n","        [-0.0625, -0.0625,  0.0625,  0.2500, -0.3125, -0.0625, -0.0625,  0.0000,\n","          0.0625,  0.3125, -0.0000,  0.0625, -0.1250,  0.0000, -0.6875, -0.0000],\n","        [-0.1875, -0.1250, -0.0625, -0.5625, -0.3125, -0.3125,  0.1250,  0.1875,\n","          0.0625,  0.4375, -0.1250, -0.3125, -0.1250,  0.4375, -0.0000, -0.0625],\n","        [ 0.0625, -0.1250,  0.2500,  0.2500,  0.0625,  0.2500,  0.0625,  0.0000,\n","         -0.0625, -0.2500, -0.0000,  0.0625,  0.1875, -0.1875,  0.5625, -0.0625],\n","        [-0.1250, -0.0625,  0.7500,  0.1875, -0.5000, -0.5000, -0.1250, -0.0625,\n","          0.0000,  0.1250,  0.0000, -0.0000, -0.2500,  0.1875,  0.0000,  0.0625]],\n","       grad_fn=<IntegerQuantizeBackward>)\n","Output for original module: tensor([-1.0319,  1.1463,  0.0968,  0.9599,  0.4183, -1.8842, -0.1512, -1.0026,\n","         0.4588, -0.7618,  0.4672,  0.1299, -0.0755,  0.0975,  0.7095, -1.2043,\n","         0.2164, -1.8723,  0.0123,  0.8602,  0.7577,  0.7103, -0.7304,  0.2014,\n","        -0.0517, -1.4755,  0.3438, -1.3266, -0.2505,  0.2416,  0.8173,  0.4878],\n","       grad_fn=<ViewBackward0>)\n","Output for quantized module: tensor([-1.0234,  1.1836,  0.0859,  0.9297,  0.4258, -1.9453, -0.1875, -1.0391,\n","         0.4727, -0.7227,  0.4492,  0.1094, -0.1680,  0.0742,  0.6523, -1.1875,\n","         0.1758, -1.9375,  0.0703,  0.7852,  0.8906,  0.6367, -0.7734,  0.2656,\n","        -0.0391, -1.4180,  0.3750, -1.3789, -0.2852,  0.1719,  0.9062,  0.4219],\n","       grad_fn=<ViewBackward0>)\n","Difference found at name: seq_blocks_5, MASE type: module_related_func, MASE operation: linear\n","Original module: <class 'torch.nn.modules.linear.Linear'> --> New module: <class 'chop.passes.graph.transforms.quantize.quantized_modules.linear.LinearInteger'>\n","Precision of original module: [32]\n","Precision of modified module: [8, 4]\n","Type of original module: float\n","Tyoe of modified module: integer\n","Weight of original module: Parameter containing:\n","tensor([[-9.3865e-03, -4.4277e-02, -8.8426e-02,  1.8823e-01, -9.4592e-02,\n","          3.5020e-02,  2.1780e-01, -2.4669e-03,  4.5797e-01,  2.8514e-02,\n","          2.0174e-01,  2.3578e-01, -5.1667e-02, -1.0595e-01,  6.3118e-01,\n","         -4.6962e-02,  4.3133e-01,  6.3068e-02,  2.6001e-01,  3.0305e-01,\n","         -3.3856e-02, -9.7126e-02,  3.2829e-01,  1.1802e-01, -2.5944e-01,\n","         -8.8591e-02,  1.1392e-01, -1.6266e-01, -6.2965e-02, -9.8105e-02,\n","          1.8416e-01,  4.7282e-02],\n","        [ 2.0575e-01, -1.1983e-01,  1.4202e-01, -1.3424e-01,  1.4884e-02,\n","         -1.8929e-01,  2.3364e-02,  8.4770e-02,  8.2159e-02, -3.4340e-01,\n","          6.7766e-01,  1.5137e-01, -4.6183e-04, -7.3303e-02,  1.4035e-01,\n","         -3.9131e-01, -2.2095e-02,  4.9189e-03,  3.3001e-01, -7.0730e-02,\n","         -2.9368e-01,  3.8562e-02,  1.7754e-01,  1.8061e-01, -2.0838e-01,\n","         -1.3642e-02, -1.9942e-01, -1.0518e-01, -1.1185e-01, -1.8169e-01,\n","         -8.8553e-02, -3.2384e-01],\n","        [-4.6750e-01, -9.8992e-02, -4.4565e-01, -1.4682e-01,  8.7814e-02,\n","         -1.2514e-01, -5.0218e-01, -9.7685e-03, -1.7838e-01, -2.8225e-01,\n","          2.8413e-01,  4.3216e-01, -2.3323e-03, -2.6992e-01, -3.9600e-03,\n","         -8.7342e-02, -1.7323e-01, -7.4453e-02,  1.1645e-01,  1.0299e-01,\n","         -7.5567e-02,  5.3015e-02,  6.3292e-03,  7.8200e-02, -1.4271e-01,\n","          1.5661e-01,  1.8347e-01, -8.4025e-02, -2.7845e-01, -1.6046e-01,\n","         -5.3043e-01,  6.4706e-02],\n","        [ 1.9075e-03,  1.7508e-01, -2.1856e-01, -5.6208e-02,  9.4070e-02,\n","          8.0784e-02,  5.2537e-02,  4.0826e-01, -2.9825e-01, -6.4410e-02,\n","         -1.5905e-02,  1.3620e-01,  2.9638e-01, -1.6196e-01,  1.6955e-01,\n","          3.0595e-02, -2.0349e-01, -3.2091e-02,  2.1295e-01, -9.3390e-02,\n","         -1.6564e-01,  3.0578e-01, -2.9738e-02, -7.8846e-02, -3.1073e-01,\n","         -5.7953e-02, -2.2082e-01,  1.9189e-01, -3.8443e-01, -1.5066e-01,\n","         -3.1944e-01, -1.5898e-01],\n","        [ 1.8773e-01, -4.6258e-03,  2.2979e-01,  2.6003e-01, -1.4713e-01,\n","          6.3969e-02,  2.1206e-01, -3.0467e-01,  7.1338e-02, -1.5588e-01,\n","          3.7697e-02,  5.2116e-02,  3.8525e-02, -1.3840e-01, -2.1112e-01,\n","         -1.6042e-01, -1.2093e-01,  1.2566e-02, -1.5365e-01, -2.7011e-01,\n","          2.4980e-01, -3.0707e-02, -1.7362e-01,  2.4047e-02, -2.1174e-03,\n","          3.4720e-01, -8.9778e-02,  2.3231e-02,  2.3471e-01, -4.4224e-01,\n","          7.5950e-03,  3.0610e-01],\n","        [-3.4325e-03,  3.2792e-03,  3.2954e-01, -2.3291e-01, -1.6128e-01,\n","          1.1724e-01,  8.0590e-02,  2.1623e-02, -4.5424e-02,  1.9170e-01,\n","         -5.5544e-01, -6.5342e-01,  1.7312e-01,  6.6225e-02, -2.1965e-02,\n","          1.8460e-01,  7.7343e-03,  1.9945e-02, -4.2701e-01, -2.4864e-01,\n","          4.1685e-02,  1.1294e-01, -6.7226e-02, -2.4260e-01,  2.3376e-01,\n","         -1.6879e-01, -2.7306e-01,  2.1063e-01,  2.1846e-02, -7.1037e-03,\n","          8.2314e-02, -2.6599e-01],\n","        [ 4.4311e-01,  2.8163e-02,  2.0818e-02, -3.0134e-01, -1.7776e-01,\n","         -9.1409e-03,  4.2316e-01, -1.9040e-01, -2.2178e-02,  1.8846e-01,\n","         -3.9496e-01, -6.2608e-01,  2.9277e-02, -4.7100e-02, -1.1189e-02,\n","          2.5146e-01,  5.5256e-02,  1.9126e-01, -2.6555e-01, -6.9054e-03,\n","          2.8238e-01,  5.7195e-02,  1.7744e-02, -1.3345e-01,  7.5751e-02,\n","         -5.1866e-03, -1.4483e-01,  1.6200e-01,  3.1059e-01,  3.2434e-01,\n","          2.1018e-02, -1.5125e-01],\n","        [-5.9292e-01, -2.2683e-01, -8.2936e-03,  2.4248e-01, -7.1847e-02,\n","         -6.8204e-02, -1.5728e-01,  2.6345e-01,  1.1583e-01,  4.7231e-02,\n","          1.0966e-01,  5.0778e-01, -1.0709e-01,  1.0782e-01,  4.9616e-02,\n","         -1.1671e-01,  4.8302e-02,  1.8871e-01,  6.9311e-03, -3.5337e-01,\n","          8.5875e-02, -3.2340e-01, -1.0179e-01,  1.2651e-01,  7.2844e-03,\n","         -8.5312e-02, -1.9999e-01, -2.3360e-01, -6.6991e-01, -2.7498e-01,\n","          6.2444e-02,  8.2281e-02],\n","        [ 1.3014e-01, -1.6110e-01,  3.7512e-02, -4.9988e-01, -2.4832e-01,\n","          3.4696e-01, -2.6105e-01, -9.1796e-02,  8.1821e-02,  6.5561e-02,\n","         -1.2247e-01, -2.4202e-01, -1.5242e-01,  3.6289e-03, -3.3293e-01,\n","          1.3575e-01,  1.5754e-01,  1.4009e-01, -4.2982e-01,  1.5967e-01,\n","          1.9082e-01, -2.9726e-01,  3.8194e-02, -5.9388e-02, -5.1681e-02,\n","          6.5240e-02,  1.0619e-01,  3.8306e-02, -1.5374e-01,  1.8831e-01,\n","          5.7314e-02, -6.5691e-02],\n","        [ 1.8097e-01,  1.6158e-01, -1.3099e-01,  7.7926e-02, -9.4715e-02,\n","         -1.4254e-01,  8.8650e-02,  1.1416e-01, -7.0758e-02,  2.0139e-01,\n","         -1.3555e-01,  1.9767e-01, -1.7834e-01,  9.5368e-03,  3.3749e-02,\n","          1.1740e-01, -1.2242e-01,  1.7881e-01, -7.1995e-02, -5.0330e-02,\n","          2.1845e-01,  4.0676e-02,  4.2460e-02, -1.6356e-01,  1.7357e-01,\n","          3.8613e-01,  2.9488e-02,  2.7349e-02,  5.3098e-01, -5.2642e-01,\n","          1.4933e-01,  3.0122e-01],\n","        [-2.0499e-01, -1.0978e-01,  2.8694e-02, -3.0228e-01,  8.4898e-03,\n","          5.2752e-02, -1.6422e-01, -7.2786e-01, -1.5077e-01,  7.1752e-02,\n","          2.6656e-01,  3.0609e-01,  8.2773e-02,  3.2811e-01,  6.4666e-02,\n","          2.6378e-01, -2.9351e-02, -1.1415e-01, -1.4518e-01,  1.5335e-01,\n","         -2.8010e-01,  2.1296e-01, -1.1282e-01, -9.1093e-03,  9.5821e-02,\n","          2.3382e-01,  1.9382e-02, -5.6365e-02, -2.0620e-01, -1.7380e-01,\n","          1.1800e-01, -3.6228e-02],\n","        [-2.7611e-01,  1.3692e-01,  1.8718e-01,  2.6855e-01,  1.9197e-01,\n","         -1.9468e-01,  9.0942e-01, -1.3441e-01, -6.1796e-02, -7.8045e-02,\n","         -1.1242e-01,  3.2650e-02,  9.2857e-02,  6.9577e-02,  2.8708e-01,\n","         -6.3972e-02,  4.8090e-02, -3.4787e-01,  2.9069e-01,  1.6052e-02,\n","         -7.0486e-02,  1.6921e-01, -3.8358e-02, -1.0990e-01,  1.4707e-02,\n","         -2.0007e-01, -2.5576e-01,  7.0879e-02,  3.4355e-01, -5.0814e-02,\n","          4.8319e-02, -2.1685e-01],\n","        [ 4.2539e-01,  4.8452e-02, -1.0208e-01, -3.5201e-01,  8.2046e-02,\n","          3.6513e-01,  1.0789e-01, -1.5835e-01, -3.0224e-02,  5.6889e-02,\n","          6.2310e-02,  3.0833e-01, -4.4278e-01, -2.0379e-01,  3.9979e-02,\n","          2.5806e-01,  1.8289e-01, -1.9933e-02, -3.1803e-01, -2.2957e-01,\n","          1.3553e-01, -4.5036e-02, -2.7182e-01, -3.9562e-02, -2.9703e-01,\n","          3.5550e-01, -5.4582e-02, -6.1746e-03, -3.6585e-02, -2.0899e-01,\n","          1.1388e-01,  8.1310e-02],\n","        [ 1.5373e-01, -2.2222e-01,  1.1998e-01, -4.6373e-01, -2.8989e-01,\n","          2.3742e-01, -2.6151e-01,  3.6157e-03,  1.1899e-01,  1.0470e-01,\n","         -6.8698e-02, -2.2304e-03, -8.4089e-02, -4.0621e-01, -1.3850e-01,\n","          1.2228e-01,  3.1920e-01,  1.8295e-01,  1.0883e-03, -6.0665e-02,\n","          5.1999e-03, -2.0060e-01,  3.4556e-01, -2.3749e-01, -4.0461e-01,\n","         -9.9985e-02, -1.8850e-02,  1.4688e-01, -2.5601e-01,  1.2115e-01,\n","         -2.3375e-01, -1.6331e-01],\n","        [-4.6662e-01,  1.4004e-01,  1.9885e-01, -7.8071e-02,  1.2727e-01,\n","          4.4079e-02, -1.3995e-01,  1.5461e-01,  2.0460e-01, -1.4791e-02,\n","         -4.9493e-02, -6.7903e-02,  1.2199e-01,  2.3286e-01,  1.4072e-01,\n","         -6.3967e-02,  8.6811e-02, -2.0062e-01,  3.3139e-01, -2.6665e-01,\n","         -1.9718e-01,  4.1145e-02, -5.5247e-02,  2.9052e-01,  7.7129e-02,\n","         -2.9981e-01, -3.1620e-01,  8.8812e-02, -5.0103e-01,  7.2377e-02,\n","         -1.9732e-01, -1.1956e-02],\n","        [ 2.3934e-01, -1.3187e-01,  8.7947e-02, -6.4035e-02, -2.5076e-01,\n","          2.1198e-01,  8.5156e-02,  3.1132e-01,  6.4338e-02,  1.3417e-01,\n","         -3.1993e-02, -3.9797e-01,  1.3521e-02, -6.1146e-02, -1.0846e-01,\n","         -9.3476e-02, -1.0295e-01,  4.0461e-01,  9.5553e-02, -2.2581e-01,\n","         -8.3627e-02, -8.0458e-02,  8.0132e-02, -4.4833e-02, -1.1699e-01,\n","         -1.2352e-01, -5.0132e-01,  1.8802e-01, -1.3038e-01,  1.7699e-01,\n","         -2.3401e-01, -1.9644e-01]], requires_grad=True)\n","Weight of quantized module: tensor([[-0.0000, -0.0625, -0.0625,  0.1875, -0.1250,  0.0625,  0.1875, -0.0000,\n","          0.4375,  0.0000,  0.1875,  0.2500, -0.0625, -0.1250,  0.6250, -0.0625,\n","          0.4375,  0.0625,  0.2500,  0.3125, -0.0625, -0.1250,  0.3125,  0.1250,\n","         -0.2500, -0.0625,  0.1250, -0.1875, -0.0625, -0.1250,  0.1875,  0.0625],\n","        [ 0.1875, -0.1250,  0.1250, -0.1250,  0.0000, -0.1875,  0.0000,  0.0625,\n","          0.0625, -0.3125,  0.6875,  0.1250, -0.0000, -0.0625,  0.1250, -0.3750,\n","         -0.0000,  0.0000,  0.3125, -0.0625, -0.3125,  0.0625,  0.1875,  0.1875,\n","         -0.1875, -0.0000, -0.1875, -0.1250, -0.1250, -0.1875, -0.0625, -0.3125],\n","        [-0.4375, -0.1250, -0.4375, -0.1250,  0.0625, -0.1250, -0.5000, -0.0000,\n","         -0.1875, -0.3125,  0.3125,  0.4375, -0.0000, -0.2500, -0.0000, -0.0625,\n","         -0.1875, -0.0625,  0.1250,  0.1250, -0.0625,  0.0625,  0.0000,  0.0625,\n","         -0.1250,  0.1875,  0.1875, -0.0625, -0.2500, -0.1875, -0.5000,  0.0625],\n","        [ 0.0000,  0.1875, -0.1875, -0.0625,  0.1250,  0.0625,  0.0625,  0.4375,\n","         -0.3125, -0.0625, -0.0000,  0.1250,  0.3125, -0.1875,  0.1875,  0.0000,\n","         -0.1875, -0.0625,  0.1875, -0.0625, -0.1875,  0.3125, -0.0000, -0.0625,\n","         -0.3125, -0.0625, -0.2500,  0.1875, -0.3750, -0.1250, -0.3125, -0.1875],\n","        [ 0.1875, -0.0000,  0.2500,  0.2500, -0.1250,  0.0625,  0.1875, -0.3125,\n","          0.0625, -0.1250,  0.0625,  0.0625,  0.0625, -0.1250, -0.1875, -0.1875,\n","         -0.1250,  0.0000, -0.1250, -0.2500,  0.2500, -0.0000, -0.1875,  0.0000,\n","         -0.0000,  0.3750, -0.0625,  0.0000,  0.2500, -0.4375,  0.0000,  0.3125],\n","        [-0.0000,  0.0000,  0.3125, -0.2500, -0.1875,  0.1250,  0.0625,  0.0000,\n","         -0.0625,  0.1875, -0.5625, -0.6250,  0.1875,  0.0625, -0.0000,  0.1875,\n","          0.0000,  0.0000, -0.4375, -0.2500,  0.0625,  0.1250, -0.0625, -0.2500,\n","          0.2500, -0.1875, -0.2500,  0.1875,  0.0000, -0.0000,  0.0625, -0.2500],\n","        [ 0.4375,  0.0000,  0.0000, -0.3125, -0.1875, -0.0000,  0.4375, -0.1875,\n","         -0.0000,  0.1875, -0.3750, -0.6250,  0.0000, -0.0625, -0.0000,  0.2500,\n","          0.0625,  0.1875, -0.2500, -0.0000,  0.3125,  0.0625,  0.0000, -0.1250,\n","          0.0625, -0.0000, -0.1250,  0.1875,  0.3125,  0.3125,  0.0000, -0.1250],\n","        [-0.5625, -0.2500, -0.0000,  0.2500, -0.0625, -0.0625, -0.1875,  0.2500,\n","          0.1250,  0.0625,  0.1250,  0.5000, -0.1250,  0.1250,  0.0625, -0.1250,\n","          0.0625,  0.1875,  0.0000, -0.3750,  0.0625, -0.3125, -0.1250,  0.1250,\n","          0.0000, -0.0625, -0.1875, -0.2500, -0.6875, -0.2500,  0.0625,  0.0625],\n","        [ 0.1250, -0.1875,  0.0625, -0.5000, -0.2500,  0.3750, -0.2500, -0.0625,\n","          0.0625,  0.0625, -0.1250, -0.2500, -0.1250,  0.0000, -0.3125,  0.1250,\n","          0.1875,  0.1250, -0.4375,  0.1875,  0.1875, -0.3125,  0.0625, -0.0625,\n","         -0.0625,  0.0625,  0.1250,  0.0625, -0.1250,  0.1875,  0.0625, -0.0625],\n","        [ 0.1875,  0.1875, -0.1250,  0.0625, -0.1250, -0.1250,  0.0625,  0.1250,\n","         -0.0625,  0.1875, -0.1250,  0.1875, -0.1875,  0.0000,  0.0625,  0.1250,\n","         -0.1250,  0.1875, -0.0625, -0.0625,  0.1875,  0.0625,  0.0625, -0.1875,\n","          0.1875,  0.3750,  0.0000,  0.0000,  0.5000, -0.5000,  0.1250,  0.3125],\n","        [-0.1875, -0.1250,  0.0000, -0.3125,  0.0000,  0.0625, -0.1875, -0.7500,\n","         -0.1250,  0.0625,  0.2500,  0.3125,  0.0625,  0.3125,  0.0625,  0.2500,\n","         -0.0000, -0.1250, -0.1250,  0.1250, -0.2500,  0.1875, -0.1250, -0.0000,\n","          0.1250,  0.2500,  0.0000, -0.0625, -0.1875, -0.1875,  0.1250, -0.0625],\n","        [-0.2500,  0.1250,  0.1875,  0.2500,  0.1875, -0.1875,  0.9375, -0.1250,\n","         -0.0625, -0.0625, -0.1250,  0.0625,  0.0625,  0.0625,  0.3125, -0.0625,\n","          0.0625, -0.3750,  0.3125,  0.0000, -0.0625,  0.1875, -0.0625, -0.1250,\n","          0.0000, -0.1875, -0.2500,  0.0625,  0.3125, -0.0625,  0.0625, -0.1875],\n","        [ 0.4375,  0.0625, -0.1250, -0.3750,  0.0625,  0.3750,  0.1250, -0.1875,\n","         -0.0000,  0.0625,  0.0625,  0.3125, -0.4375, -0.1875,  0.0625,  0.2500,\n","          0.1875, -0.0000, -0.3125, -0.2500,  0.1250, -0.0625, -0.2500, -0.0625,\n","         -0.3125,  0.3750, -0.0625, -0.0000, -0.0625, -0.1875,  0.1250,  0.0625],\n","        [ 0.1250, -0.2500,  0.1250, -0.4375, -0.3125,  0.2500, -0.2500,  0.0000,\n","          0.1250,  0.1250, -0.0625, -0.0000, -0.0625, -0.3750, -0.1250,  0.1250,\n","          0.3125,  0.1875,  0.0000, -0.0625,  0.0000, -0.1875,  0.3750, -0.2500,\n","         -0.3750, -0.1250, -0.0000,  0.1250, -0.2500,  0.1250, -0.2500, -0.1875],\n","        [-0.4375,  0.1250,  0.1875, -0.0625,  0.1250,  0.0625, -0.1250,  0.1250,\n","          0.1875, -0.0000, -0.0625, -0.0625,  0.1250,  0.2500,  0.1250, -0.0625,\n","          0.0625, -0.1875,  0.3125, -0.2500, -0.1875,  0.0625, -0.0625,  0.3125,\n","          0.0625, -0.3125, -0.3125,  0.0625, -0.5000,  0.0625, -0.1875, -0.0000],\n","        [ 0.2500, -0.1250,  0.0625, -0.0625, -0.2500,  0.1875,  0.0625,  0.3125,\n","          0.0625,  0.1250, -0.0625, -0.3750,  0.0000, -0.0625, -0.1250, -0.0625,\n","         -0.1250,  0.3750,  0.1250, -0.2500, -0.0625, -0.0625,  0.0625, -0.0625,\n","         -0.1250, -0.1250, -0.5000,  0.1875, -0.1250,  0.1875, -0.2500, -0.1875]],\n","       grad_fn=<IntegerQuantizeBackward>)\n","Output for original module: tensor([ 0.8304, -0.3652, -0.7161, -1.5028,  0.1177,  0.9222, -0.2941,  1.3581,\n","         1.9159, -0.0045, -0.1775, -1.9102,  0.1614,  2.4037,  0.3516,  0.5788],\n","       grad_fn=<ViewBackward0>)\n","Output for quantized module: tensor([ 0.9805, -0.4531, -0.8594, -1.4102,  0.1133,  0.9297, -0.3516,  1.3516,\n","         2.0273, -0.0664,  0.0117, -1.8711,  0.2500,  2.3516,  0.3438,  0.4492],\n","       grad_fn=<ViewBackward0>)\n","Difference found at name: seq_blocks_8, MASE type: module_related_func, MASE operation: linear\n","Original module: <class 'torch.nn.modules.linear.Linear'> --> New module: <class 'chop.passes.graph.transforms.quantize.quantized_modules.linear.LinearInteger'>\n","Precision of original module: [32]\n","Precision of modified module: [8, 4]\n","Type of original module: float\n","Tyoe of modified module: integer\n","Weight of original module: Parameter containing:\n","tensor([[ 2.8503e-01, -2.4187e-01,  2.9632e-01, -2.3284e-01, -9.7513e-02,\n","         -3.4525e-01,  1.5537e-01,  6.0373e-01, -5.3559e-02, -5.6506e-02,\n","          1.7504e-01,  6.7563e-02,  1.4649e-01, -8.2523e-02,  1.6492e-01,\n","         -7.7725e-02],\n","        [-6.6299e-02, -1.8386e-01,  1.4257e-01,  1.7533e-01, -2.6672e-01,\n","          4.0419e-02, -4.6937e-01,  3.2683e-01,  1.4349e-01, -3.1104e-01,\n","         -7.6031e-02, -2.6594e-01, -1.9770e-01, -1.2898e-04,  4.3715e-01,\n","          9.2998e-03],\n","        [-3.2757e-01, -1.0854e-01, -2.9945e-01,  3.1590e-01,  1.7741e-01,\n","         -7.1081e-02, -1.1005e-01, -3.2033e-01, -2.7057e-01,  1.2269e-01,\n","         -5.1512e-01, -1.7390e-01,  1.6326e-01, -4.7843e-03,  1.1504e-01,\n","          1.0025e-01],\n","        [-1.0666e-01,  9.3104e-03, -5.7052e-01,  2.1438e-01, -1.5043e-01,\n","         -4.7999e-02, -2.4174e-01,  5.1141e-02, -3.3691e-02,  6.6162e-02,\n","         -2.5410e-01, -7.9331e-02,  4.1718e-02, -1.1652e-03, -1.3933e-01,\n","         -1.2057e-01],\n","        [-1.3861e-01,  1.7030e-01, -2.7611e-03, -1.9987e-01, -9.3793e-02,\n","         -5.0584e-03,  2.2270e-01,  7.9652e-02,  1.0396e-01, -4.9225e-01,\n","          4.4972e-01, -1.2125e-01,  2.2268e-02, -3.3787e-01,  1.4471e-01,\n","          1.7229e-01],\n","        [-2.2287e-01, -4.9355e-01,  2.3441e-01, -2.8239e-01,  1.3962e-01,\n","         -8.0488e-02,  1.1470e-02, -2.0332e-02, -4.4618e-01,  9.8612e-02,\n","          6.9981e-02,  1.9970e-01, -3.3735e-01, -5.0068e-01,  4.3883e-02,\n","         -2.0797e-01],\n","        [-3.5550e-02, -3.1992e-03, -2.0386e-01,  4.2311e-01, -3.1807e-02,\n","         -3.3169e-01, -3.2983e-01, -6.6052e-02, -4.5152e-02,  1.2183e-02,\n","         -1.1786e-01, -2.4173e-01,  1.4775e-01, -1.6786e-01,  1.1705e-01,\n","         -1.7340e-01],\n","        [-5.4315e-02, -1.5970e-01,  1.5990e-01,  3.2229e-01, -1.5139e-01,\n","         -2.7702e-01,  2.1649e-02, -1.4650e-01, -3.8675e-01,  1.2521e-01,\n","          1.4858e-01, -4.0223e-01,  1.6205e-01,  5.1013e-02,  3.5823e-01,\n","         -3.1633e-01]], requires_grad=True)\n","Weight of quantized module: tensor([[ 0.3125, -0.2500,  0.3125, -0.2500, -0.1250, -0.3750,  0.1250,  0.6250,\n","         -0.0625, -0.0625,  0.1875,  0.0625,  0.1250, -0.0625,  0.1875, -0.0625],\n","        [-0.0625, -0.1875,  0.1250,  0.1875, -0.2500,  0.0625, -0.5000,  0.3125,\n","          0.1250, -0.3125, -0.0625, -0.2500, -0.1875, -0.0000,  0.4375,  0.0000],\n","        [-0.3125, -0.1250, -0.3125,  0.3125,  0.1875, -0.0625, -0.1250, -0.3125,\n","         -0.2500,  0.1250, -0.5000, -0.1875,  0.1875, -0.0000,  0.1250,  0.1250],\n","        [-0.1250,  0.0000, -0.5625,  0.1875, -0.1250, -0.0625, -0.2500,  0.0625,\n","         -0.0625,  0.0625, -0.2500, -0.0625,  0.0625, -0.0000, -0.1250, -0.1250],\n","        [-0.1250,  0.1875, -0.0000, -0.1875, -0.1250, -0.0000,  0.2500,  0.0625,\n","          0.1250, -0.5000,  0.4375, -0.1250,  0.0000, -0.3125,  0.1250,  0.1875],\n","        [-0.2500, -0.5000,  0.2500, -0.3125,  0.1250, -0.0625,  0.0000, -0.0000,\n","         -0.4375,  0.1250,  0.0625,  0.1875, -0.3125, -0.5000,  0.0625, -0.1875],\n","        [-0.0625, -0.0000, -0.1875,  0.4375, -0.0625, -0.3125, -0.3125, -0.0625,\n","         -0.0625,  0.0000, -0.1250, -0.2500,  0.1250, -0.1875,  0.1250, -0.1875],\n","        [-0.0625, -0.1875,  0.1875,  0.3125, -0.1250, -0.2500,  0.0000, -0.1250,\n","         -0.3750,  0.1250,  0.1250, -0.3750,  0.1875,  0.0625,  0.3750, -0.3125]],\n","       grad_fn=<IntegerQuantizeBackward>)\n","Output for original module: tensor([ 1.4410,  1.1293, -0.2024,  0.2862, -0.4875, -0.5078, -0.1929, -0.2872],\n","       grad_fn=<ViewBackward0>)\n","Output for quantized module: tensor([ 1.5039,  1.0039, -0.1680,  0.3125, -0.5273, -0.4258, -0.2305, -0.2617],\n","       grad_fn=<ViewBackward0>)\n","Difference found at name: seq_blocks_11, MASE type: module_related_func, MASE operation: linear\n","Original module: <class 'torch.nn.modules.linear.Linear'> --> New module: <class 'chop.passes.graph.transforms.quantize.quantized_modules.linear.LinearInteger'>\n","Precision of original module: [32]\n","Precision of modified module: [8, 4]\n","Type of original module: float\n","Tyoe of modified module: integer\n","Weight of original module: Parameter containing:\n","tensor([[ 0.2618, -0.5175, -0.3254,  0.0716,  0.1549, -0.0914, -0.0130,  0.0812],\n","        [-0.0635, -0.2807, -0.1034,  0.2355,  0.4414,  0.1661, -0.1936,  0.0377],\n","        [-0.0527,  0.1949,  0.1404, -0.5508,  0.1216, -0.0447,  0.3747, -0.1527],\n","        [-0.0363,  0.0913,  0.0132, -0.1101,  0.1022, -0.0063,  0.4753, -0.5509],\n","        [-0.2986, -0.4857,  0.3317,  0.0886, -0.2743, -0.4919, -0.1644,  0.2120]],\n","       requires_grad=True)\n","Weight of quantized module: tensor([[ 0.2500, -0.5000, -0.3125,  0.0625,  0.1250, -0.0625, -0.0000,  0.0625],\n","        [-0.0625, -0.2500, -0.1250,  0.2500,  0.4375,  0.1875, -0.1875,  0.0625],\n","        [-0.0625,  0.1875,  0.1250, -0.5625,  0.1250, -0.0625,  0.3750, -0.1250],\n","        [-0.0625,  0.0625,  0.0000, -0.1250,  0.1250, -0.0000,  0.5000, -0.5625],\n","        [-0.3125, -0.5000,  0.3125,  0.0625, -0.2500, -0.5000, -0.1875,  0.1875]],\n","       grad_fn=<IntegerQuantizeBackward>)\n","Output for original module: tensor([-0.0572, -0.1932, -0.1876, -1.0639,  1.2111], grad_fn=<ViewBackward0>)\n","Output for quantized module: tensor([-0.0273, -0.2109, -0.1289, -1.0664,  1.2344], grad_fn=<ViewBackward0>)\n"]}],"source":["# iterate over original and modified graphs\n","for ori_n, n in zip(ori_mg.fx_graph.nodes, mg.fx_graph.nodes):\n","    # check if the original node and the modified node are the same\n","    # if they arent, then it means that that node has been quantized\n","    if type(get_node_actual_target(n)) != type(get_node_actual_target(ori_n)):\n","        # retrieve the original module from the node\n","        ori_module = get_node_actual_target(ori_n)\n","        # retrieve the quantized module from the node\n","        quant_module = get_node_actual_target(n)\n","\n","        print(f'Difference found at name: {n.name}, '\n","              f'MASE type: {get_mase_type(n)}, MASE operation: {get_mase_op(n)}\\n'\n","              f'Original module: {type(ori_module)} --> '\n","              f'New module: {type(quant_module)}')\n","        \n","        # Get the precision and types of the weights of the nodes from their metadata\n","        mg_typemg_precision = n.meta[\"mase\"].parameters[\"common\"][\"args\"][\"weight\"][\"precision\"]\n","        ori_mg_precision = ori_n.meta[\"mase\"].parameters[\"common\"][\"args\"][\"weight\"][\"precision\"]\n","\n","        mg_type = n.meta[\"mase\"].parameters[\"common\"][\"args\"][\"weight\"][\"type\"]\n","        ori_mg_type = ori_n.meta[\"mase\"].parameters[\"common\"][\"args\"][\"weight\"][\"type\"]\n","\n","        print(f'Precision of original module: {ori_mg_precision}')\n","        print(f'Precision of modified module: {mg_precision}')\n","\n","        print(f'Type of original module: {ori_mg_type}')\n","        print(f'Tyoe of modified module: {mg_type}')\n","\n","        # print the weights of the original and quantized modules\n","        print(f'Weight of original module: {ori_module.weight}')\n","        quantized_weights = quant_module.w_quantizer(ori_module.weight)\n","        print(f'Weight of quantized module: {quantized_weights}')\n","\n","        # generate a test input tensor based on the input feature size of the quantized module\n","        test_input = torch.randn(quant_module.in_features)\n","\n","        # apply the original and quantized modules to the test input and print the outputs\n","        print(f'Output for original module: {ori_module(test_input)}')\n","        print(f'Output for quantized module: {quant_module(test_input)}')\n"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"torch","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
