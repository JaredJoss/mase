When you stack multiple linear layers without non-linear activation functions in between, the composition of these layers remains a linear transformation. This means that no matter how many layers you add, the overall operation can be represented as a single linear transformation. As a result, the network would lack the capacity to learn complex, non-linear relationships between input and output.

The purpose of activation functions such as ReLU, sigmoid, or tanh is to introduce non-linearities into the network. These non-linearities allow the neural network to learn and model more complex patterns and relationships in the data. Without them, the network would be limited to linear transformations, making it unable to capture the intricacies of many real-world problems.

# Question 1

# Question 2

# Question 3

# Question 4
